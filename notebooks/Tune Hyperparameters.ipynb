{"cells":[{"cell_type":"markdown","source":["# BCI AVM Hypertuning\n### Training the Machine Learning Models on Tabular Data: \n\nThis notebook covers the following steps:\n- Import data from AWS\n- Visualize the data using Seaborn and matplotlib\n- Run a parallel hyperparameter sweep to train machine learning models on the dataset\n- Explore the results of the hyperparameter sweep with MLflow\n- Register the best performing model in MLflow\n\n## Requirements\nThis notebook requires Databricks Runtime for Machine Learning or a similar spark/pyspark enabled environment setup locally (or elsewhere).  \nIf you are using Databricks Runtime 7.3 LTS ML or below, you must update the CloudPickle library using the commands in the following cell."],"metadata":{}},{"cell_type":"code","source":["import cloudpickle"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["## Importing Data\n  \nIn this section, you download a dataset from the web (AWS S3 Bucket).\n\n1. Ensure that you have installed `bciavm` using `pip install bciavm` in your local machine or on Databricks."],"metadata":{}},{"cell_type":"code","source":["import io\nfrom bciavm.core.config import your_bucket\nfrom bciavm.utils.bci_utils import ReadParquetFile, get_postcodeOutcode_from_postcode, get_postcodeArea_from_outcode, drop_outliers, preprocess_data\nimport pandas as pd\nimport bciavm\nfrom sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\nimport sys\nimport hyperopt\nfrom hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK\nfrom hyperopt.pyll import scope\nfrom math import exp\nimport mlflow\nimport os\nimport gc\nfrom bciavm.pipelines import RegressionPipeline\nimport numpy as np"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2021-06-15 01:07:28,134 featuretools - WARNING    Featuretools failed to load plugin nlp_primitives from library nlp_primitives. For a full stack trace, set logging to debug.\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["MLFLOW_TRACKING_URI = os.environ[\"MLFLOW_TRACKING_URI\"]\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\nmlflow.set_experiment('/Users/mike.casale@blockchainclimate.org/Experiments/tune-hyperparameters')"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">INFO: &#39;/Users/mike.casale@blockchainclimate.org/Experiments/tune-hyperparameters&#39; does not exist. Creating a new experiment\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["dfPricesEpc = pd.DataFrame()\ndfPrices = pd.DataFrame()\n\nyearArray = ['2020', '2019']\nfor year in yearArray:\n    singlePriceEpcFile = pd.DataFrame(ReadParquetFile(your_bucket, 'epc_price_data/byDate/2021-02-04/parquet/' + year))\n    dfPricesEpc = dfPricesEpc.append(singlePriceEpcFile)\n\ndfPricesEpc['POSTCODE_OUTCODE'] = dfPricesEpc['Postcode'].apply(get_postcodeOutcode_from_postcode)\ndfPricesEpc['POSTCODE_AREA'] = dfPricesEpc['POSTCODE_OUTCODE'].apply(get_postcodeArea_from_outcode)\ndfPricesEpc.groupby('TypeOfMatching_m').count()['Postcode']"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: TypeOfMatching_m\n1. Address Matched            699206\n2. Address Matched No Spec     26055\n3. No in Address Matched      325243\n4. No match                   339476\nName: Postcode, dtype: int64</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["## Preprocessing Data\nPrior to training a model, check for missing values and split the data into training and validation sets."],"metadata":{}},{"cell_type":"code","source":["for n in range(100):\n  try: \n    X_train, X_test, y_train, y_test = bciavm.utils.bci_utils.preprocess_data(\n      dfPricesEpc.rename({'Postcode':'POSTCODE'},axis=1).sample(50000))\n    break\n  except: pass\n  \nX_train"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: </div>"]}},{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unit_indx</th>\n      <th>POSTCODE</th>\n      <th>POSTCODE_OUTCODE</th>\n      <th>POSTTOWN_e</th>\n      <th>PROPERTY_TYPE_e</th>\n      <th>TOTAL_FLOOR_AREA_e</th>\n      <th>NUMBER_HEATED_ROOMS_e</th>\n      <th>FLOOR_LEVEL_e</th>\n      <th>Latitude_m</th>\n      <th>Longitude_m</th>\n      <th>POSTCODE_AREA</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8917</th>\n      <td>27701</td>\n      <td>DL10 7DL</td>\n      <td>DL10</td>\n      <td>RICHMOND</td>\n      <td>House</td>\n      <td>87.00</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>54.407468</td>\n      <td>-1.718436</td>\n      <td>DL</td>\n    </tr>\n    <tr>\n      <th>1703</th>\n      <td>10829</td>\n      <td>CF83 3QE</td>\n      <td>CF83</td>\n      <td>CAERPHILLY</td>\n      <td>House</td>\n      <td>55.00</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>51.583956</td>\n      <td>-3.204553</td>\n      <td>CF</td>\n    </tr>\n    <tr>\n      <th>15156</th>\n      <td>16802</td>\n      <td>NG18 4YD</td>\n      <td>NG18</td>\n      <td>MANSFIELD</td>\n      <td>Flat</td>\n      <td>70.00</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>53.134176</td>\n      <td>-1.182214</td>\n      <td>NG</td>\n    </tr>\n    <tr>\n      <th>20813</th>\n      <td>18876</td>\n      <td>BH24 2LZ</td>\n      <td>BH24</td>\n      <td>RINGWOOD</td>\n      <td>Bungalow</td>\n      <td>279.00</td>\n      <td>8.0</td>\n      <td>0.0</td>\n      <td>50.839537</td>\n      <td>-1.837949</td>\n      <td>BH</td>\n    </tr>\n    <tr>\n      <th>10087</th>\n      <td>11472</td>\n      <td>SO40 8US</td>\n      <td>SO40</td>\n      <td>SOUTHAMPTON</td>\n      <td>Bungalow</td>\n      <td>56.06</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>50.921369</td>\n      <td>-1.514176</td>\n      <td>SO</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11443</th>\n      <td>26899</td>\n      <td>BB11 4LX</td>\n      <td>BB11</td>\n      <td>BURNLEY</td>\n      <td>House</td>\n      <td>72.45</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>53.785870</td>\n      <td>-2.253289</td>\n      <td>BB</td>\n    </tr>\n    <tr>\n      <th>3437</th>\n      <td>21339</td>\n      <td>CB1 2NU</td>\n      <td>CB1</td>\n      <td>CAMBRIDGE</td>\n      <td>House</td>\n      <td>54.00</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>52.200276</td>\n      <td>0.140023</td>\n      <td>CB</td>\n    </tr>\n    <tr>\n      <th>8888</th>\n      <td>15256</td>\n      <td>LS7 4EF</td>\n      <td>LS7</td>\n      <td>LEEDS</td>\n      <td>House</td>\n      <td>170.00</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>53.815708</td>\n      <td>-1.530616</td>\n      <td>LS</td>\n    </tr>\n    <tr>\n      <th>17809</th>\n      <td>12897</td>\n      <td>TS25 5QQ</td>\n      <td>TS25</td>\n      <td>HARTLEPOOL</td>\n      <td>House</td>\n      <td>102.00</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>54.672936</td>\n      <td>-1.231791</td>\n      <td>TS</td>\n    </tr>\n    <tr>\n      <th>5330</th>\n      <td>11609</td>\n      <td>DT11 0NT</td>\n      <td>DT11</td>\n      <td>BLANDFORD FORUM</td>\n      <td>Bungalow</td>\n      <td>98.00</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>50.839248</td>\n      <td>-2.239617</td>\n      <td>DT</td>\n    </tr>\n  </tbody>\n</table>\n<p>22317 rows Ã— 11 columns</p>\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["##Hypertuning the AVM pipeline\n\nThe following code uses the `xgboost` and `scikit-learn` libraries to train a valuation model. It runs a parallel hyperparameter sweep to train multiple\nmodels in parallel, using Hyperopt and SparkTrials. The code tracks the performance of each parameter configuration with MLflow."],"metadata":{}},{"cell_type":"code","source":["version_info = sys.version_info\nPYTHON_VERSION = \"{major}.{minor}.{micro}\".format(major=version_info.major,\n                                              minor=version_info.minor,\n                                              micro=version_info.micro)\n\n\nconda_env = {'channels': ['defaults','conda-forge'],\n            'dependencies': [\n                'python={}'.format(PYTHON_VERSION),\n                'pip',\n                  {'pip': ['bciavm==1.21.5',\n                           'dask-ml',\n                           'hyperopt'\n                          ],\n                  },\n            ],\n            'name': 'mlflow-env'\n}"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["try: os.mkdir('/dbfs/FileStore/tables/avm/tuning/')\nexcept: pass"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["search_space = {\n  'stacking': hp.choice('stacking', [True, True, False]),\n  \n  #Transformer tuning\n  'numeric_impute_strategy': hp.choice('numeric_impute_strategy', [\"mean\", \"median\", \"most_frequent\"]),\n  'top_n': hp.choice('top_n', [1, 2, 3, 4, 5, 6]),\n    \n  #K nearest neighbor tuning\n  'n_neighbors': hp.choice('n_neighbors', [1, 2, 3, 4, 5, 6, 7, 8, 9]),\n  'leaf_size': hp.choice('leaf_size', [1, 2, 3, 4, 5, 6, 7]),\n  'p': 2,\n    \n  #MultiLayer Perceptron Regressor tuning\n  'activation': 'relu',\n  'solver': 'adam',\n  'batch_size': scope.int(hp.quniform('batch_size', 600, 700, 1)),\n  'alpha': hp.choice('alpha', [0.02, 0.03]),\n  'learning_rate_init': hp.choice('learning_rate_init', [0.005, 0.01, 0.1, 0.125, 0.133, 0.15]),\n  'max_iter': hp.choice('max_iter', [150, 160, 170, 180, 190, 200]),\n  'beta_1': hp.choice('beta_1', [0.2, 0.25, 0.3]),\n  'epsilon': hp.choice('epsilon', [1e-08, 1e-07, 1e-09]),\n\n  #XGBoost Regressor tuning\n  'n_estimators': scope.int(hp.quniform('n_estimators', 400, 600, 10)),\n  'max_depth': scope.int(hp.quniform('max_depth', 80, 200, 5)),\n  'learning_rate': hp.loguniform('learning_rate', -3, 0),\n  'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n  'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n  'min_child_weight': hp.choice('min_child_weight', [5, 6, 7]),\n  \n  #Final Estimator XGBoost Regressor tuning\n  'fe_n_estimators': scope.int(hp.quniform('fe_n_estimators', 100, 1000, 10)),\n  'fe_max_depth': scope.int(hp.quniform('fe_max_depth', 4, 100, 1)),\n  'fe_learning_rate': hp.loguniform('fe_learning_rate', -3, 0),\n  'fe_reg_alpha': hp.choice('fe_reg_alpha', [0.005, 0.01, 0.02, 0.03, 0.04, 0.05]),\n  'fe_reg_lambda': hp.choice('fe_reg_lambda', [0.005, 0.01, 0.02, 0.03, 0.04, 0.05]),\n  'fe_min_child_weight': hp.choice('fe_min_child_weight', [1, 2]),\n  'fe_metric':'mae',\n  'fe_objective': 'reg:squarederror',\n  'fe_seed': 123, # Set a seed for deterministic training\n}"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["def train_model(params):\n\n    mlflow.set_experiment('/Users/mike.casale@blockchainclimate.org/Experiments/tune')\n    \n    # With MLflow autologging, hyperparameters and the trained model are automatically logged to MLflow.\n    mlflow.sklearn.autolog()\n\n    with mlflow.start_run(nested=True):\n      \n        if params['stacking']:\n            class Pipeline(RegressionPipeline):\n                custom_name = 'AVM Pipeline'\n                component_graph = {  'Preprocess Transformer': [\n                                         'Preprocess Transformer'\n                                     ],\n                                     'Imputer': [\n                                         'Imputer', \n                                         'Preprocess Transformer'\n                                     ],\n                                     'One Hot Encoder': [\n                                         'One Hot Encoder', \n                                         'Imputer'\n                                     ],\n                                     'K Nearest Neighbors': [\n                                         'K Nearest Neighbors Regressor',\n                                         'One Hot Encoder'\n                                     ],\n                                     'XGB Regressor': [\n                                         'XGBoost Regressor', \n                                         'One Hot Encoder'\n                                     ],\n                                     'ML Perceptron Regressor': [\n                                         'MultiLayer Perceptron Regressor', \n                                         'One Hot Encoder'\n                                     ],\n                                     'Linear Regressor Stack': [\n                                         'Linear Regressor',\n                                         'K Nearest Neighbors',\n                                         'XGB Regressor'\n                                     ],\n                                     'Final Estimator': [\n                                         'XGBoost Regressor', \n                                         'Linear Regressor Stack',\n                                         'K Nearest Neighbors',\n                                         'XGB Regressor',\n                                         'ML Perceptron Regressor',\n                                         'One Hot Encoder'\n                                     ]}\n\n            #Uses the best params from the Hypertuning Notebook\n            #TODO: automate by reading best params from mlflow logged trials\n            parameters = {'Imputer': {'categorical_impute_strategy': 'most_frequent',\n                            'numeric_impute_strategy': params['numeric_impute_strategy'],\n                            'categorical_fill_value': None,\n                            'numeric_fill_value': None,\n                          },\n                         'One Hot Encoder': {'top_n': params['top_n'],\n                            'features_to_encode': ['agg_cat'],\n                            'categories': None,\n                            'drop': None,\n                            'handle_unknown': 'ignore',\n                            'handle_missing': 'error',\n                          },\n                         'K Nearest Neighbors': {'n_neighbors': params['n_neighbors'],\n                            'weights': 'distance',\n                            'algorithm': 'auto',\n                            'leaf_size': params['leaf_size'],\n                            'p': params['p'],\n                            'metric': 'minkowski',\n                            'n_jobs': 1\n                         },\n                         'ML Perceptron Regressor': {'activation': 'relu',\n                            'solver': 'adam',\n                            'alpha': float(params['alpha']),\n                            'batch_size': int(params['batch_size']),\n                            'learning_rate': 'constant',\n                            'learning_rate_init': float(params['learning_rate_init']),\n                            'max_iter': int(params['max_iter']),\n                            'early_stopping': True,\n                            'beta_1': float(params['beta_1']),\n                            'beta_2': 0.999,\n                            'epsilon': float(params['epsilon']),\n                            'n_iter_no_change': 10\n                         },\n                         'XGB Regressor': {'learning_rate': params['learning_rate'],\n                                    'max_depth': params['max_depth'],\n                                    'min_child_weight': params['min_child_weight'],\n                                    'reg_alpha': params['reg_alpha'],\n                                    'reg_lambda': params['reg_lambda'],\n                                    'n_estimators': params['n_estimators']\n                         },\n                         'Final Estimator': {'learning_rate': params['fe_learning_rate'],\n                                            'max_depth': params['fe_max_depth'],\n                                            'min_child_weight': params['fe_min_child_weight'],\n                                            'reg_alpha': params['fe_reg_alpha'],\n                                            'reg_lambda': params['fe_reg_lambda'],\n                                            'n_estimators': params['fe_n_estimators']\n                         }\n            }\n        \n        else:\n            class Pipeline(RegressionPipeline):\n                custom_name = 'AVM Pipeline'\n                component_graph = {  'Preprocess Transformer': [\n                                         'Preprocess Transformer'\n                                     ],\n                                     'Imputer': [\n                                         'Imputer', \n                                         'Preprocess Transformer'\n                                     ],\n                                     'One Hot Encoder': [\n                                         'One Hot Encoder', \n                                         'Imputer'\n                                     ],\n                                     'XGB Regressor': [\n                                         'XGBoost Regressor', \n                                         'One Hot Encoder'\n                                     ]}\n\n            #Uses the best params from the Hypertuning Notebook\n            #TODO: automate by reading best params from mlflow logged trials\n            parameters = {'Imputer': {'categorical_impute_strategy': 'most_frequent',\n                            'numeric_impute_strategy': params['numeric_impute_strategy'],\n                            'categorical_fill_value': None,\n                            'numeric_fill_value': None,\n                          },\n                         'One Hot Encoder': {'top_n': params['top_n'],\n                            'features_to_encode': ['agg_cat'],\n                            'categories': None,\n                            'drop': None,\n                            'handle_unknown': 'ignore',\n                            'handle_missing': 'error',\n                          },\n                         'XGB Regressor': {'learning_rate': params['fe_learning_rate'],\n                                    'max_depth': params['fe_max_depth'],\n                                    'min_child_weight': params['fe_min_child_weight'],\n                                    'reg_alpha': params['fe_reg_alpha'],\n                                    'reg_lambda': params['fe_reg_lambda'],\n                                    'n_estimators': params['fe_n_estimators']\n                         }\n            }\n            \n        for component in parameters:\n          for param in parameters[component]:\n            _component_param = component + '_' + param\n            mlflow.log_param(_component_param, parameters[component][param])\n\n        avm_pipeline = Pipeline(parameters=parameters)\n        avm_pipeline.fit(X_train, y_train)\n\n        # Compute and return trial error\n        scores = avm_pipeline.score(X_test, \n                                         y_test, \n                                         objectives=['MAPE',\n                                                   'MdAPE',\n                                                   'ExpVariance',\n                                                   'MaxError',\n                                                   'MedianAE',\n                                                   'MSE',\n                                                   'MAE',\n                                                   'R2',\n                                                   'Root Mean Squared Error'])\n        MdAPE = scores['MdAPE']\n\n        #Examine the learned feature importances output by the model as a sanity-check.\n        fi = pd.DataFrame({'feature':avm_pipeline.get_component(\"XGB Regressor\").input_feature_names,'importance':avm_pipeline.get_component(\"XGB Regressor\").feature_importance}).sort_values(by='importance', ascending=False)\n\n        #Log the feature importances output as an artifact\n        artifact_path = '/dbfs/FileStore/tables/avm/tuning/XGBoost_importance.csv'\n        fi.to_csv(artifact_path,index=False)\n        mlflow.log_artifact(artifact_path)\n\n        #Log the scoring metrics\n        mlflow.log_metric('MAPE', scores['MAPE'])\n        mlflow.log_metric('MdAPE', scores['MdAPE'])\n        mlflow.log_metric('ExpVariance', scores['ExpVariance'])\n        mlflow.log_metric('MaxError', scores['MaxError'])\n        mlflow.log_metric('MedianAE', scores['MedianAE'])\n        mlflow.log_metric('MSE', scores['MSE'])\n        mlflow.log_metric('MAE', scores['MAE'])\n        mlflow.log_metric('R2', scores['R2'])\n        mlflow.log_metric('Root Mean Squared Error', scores['Root Mean Squared Error'])\n\n        #Log an input example for future reference\n        input_example = X_train.dropna().sample(1)\n\n        #Log the mlflow model, along with the conda environment and input example\n        mlflow.sklearn.log_model(\n                             avm_pipeline,\n                             \"avm\", \n                             conda_env=conda_env,\n                             input_example=input_example\n                            )\n\n        # fmin minimizes the MdAPE (median absolute percentage error)\n        return {'status': STATUS_OK, 'loss': scores['MdAPE']}\n\n# Greater parallelism will lead to speedups, but a less optimal hyperparameter sweep. \n# A reasonable value for parallelism is the square root of max_evals.\nspark_trials = SparkTrials(parallelism=3)\n\n# Run fmin within an MLflow run context so that each hyperparameter configuration is logged as a child run of a parent\nmlflow.set_experiment('/Users/mike.casale@blockchainclimate.org/Experiments/tune')\nwith mlflow.start_run(run_name='avm_tune'):\n    best_params = fmin(\n        fn=train_model, \n        space=search_space, \n        algo=tpe.suggest, \n        #max_evals=42,\n        timeout=10800,\n        trials=spark_trials,\n        rstate=np.random.RandomState(123)\n        )"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\r  0%|          | 0/9223372036854775807 [00:00&lt;?, ?trial/s, best loss=?]\r  0%|          | 1/9223372036854775807 [08:49&lt;1356590715622856676:41:36, 529.49s/trial, best loss: 16.068350409836064]\r  0%|          | 2/9223372036854775807 [08:56&lt;955003350732570769:38:08, 372.75s/trial, best loss: 16.068350409836064] \r  0%|          | 3/9223372036854775807 [14:00&lt;902370788160533372:01:04, 352.21s/trial, best loss: 16.06722892696348] \r  0%|          | 4/9223372036854775807 [16:14&lt;734747781162453638:15:28, 286.78s/trial, best loss: 16.06722892696348]\r  0%|          | 5/9223372036854775807 [17:47&lt;585872778041062559:17:20, 228.67s/trial, best loss: 16.06722892696348]\r  0%|          | 6/9223372036854775807 [18:16&lt;431661202658606885:32:48, 168.48s/trial, best loss: 16.06722892696348]\r  0%|          | 7/9223372036854775807 [20:00&lt;382171437137226483:29:04, 149.17s/trial, best loss: 16.06722892696348]\r  0%|          | 8/9223372036854775807 [28:52&lt;676813389938833344:17:04, 264.17s/trial, best loss: 16.06722892696348]\r  0%|          | 9/9223372036854775807 [33:54&lt;706104312993762800:04:16, 275.60s/trial, best loss: 16.06722892696348]\r  0%|          | 10/9223372036854775807 [35:12&lt;554286589700938428:52:16, 216.35s/trial, best loss: 11.860074513381996]\r  0%|          | 11/9223372036854775807 [35:36&lt;405700405289304209:38:08, 158.35s/trial, best loss: 11.860074513381996]\r  0%|          | 12/9223372036854775807 [41:01&lt;534020536644116225:08:16, 208.44s/trial, best loss: 11.860074513381996]\r  0%|          | 13/9223372036854775807 [45:09&lt;564602537284382811:01:20, 220.37s/trial, best loss: 10.889102193995381]\r  0%|          | 14/9223372036854775807 [47:00&lt;480622878324941273:18:56, 187.59s/trial, best loss: 10.889102193995381]\r  0%|          | 15/9223372036854775807 [49:15&lt;440305674521477120:00:00, 171.86s/trial, best loss: 10.889102193995381]\r  0%|          | 16/9223372036854775807 [50:40&lt;373607791264871947:22:40, 145.82s/trial, best loss: 10.889102193995381]\r  0%|          | 17/9223372036854775807 [54:54&lt;456169049010165805:30:40, 178.05s/trial, best loss: 10.889102193995381]\r  0%|          | 18/9223372036854775807 [55:03&lt;326263725142656869:15:44, 127.34s/trial, best loss: 10.889102193995381]\r  0%|          | 19/9223372036854775807 [55:45&lt;260716874014031580:43:44, 101.76s/trial, best loss: 10.889102193995381]\r  0%|          | 20/9223372036854775807 [59:05&lt;336397199428706704:29:52, 131.30s/trial, best loss: 10.889102193995381]\r  0%|          | 21/9223372036854775807 [1:00:49&lt;315511521405731471:21:36, 123.15s/trial, best loss: 10.889102193995381]\r  0%|          | 22/9223372036854775807 [1:02:09&lt;282434064335626567:40:48, 110.24s/trial, best loss: 10.762487499999999]\r  0%|          | 23/9223372036854775807 [1:04:33&lt;308512771834486747:35:28, 120.42s/trial, best loss: 10.762487499999999]\r  0%|          | 24/9223372036854775807 [1:05:14&lt;247530071606503851:48:16, 96.61s/trial, best loss: 10.762487499999999] \r  0%|          | 25/9223372036854775807 [1:05:41&lt;194064691505641590:19:44, 75.75s/trial, best loss: 10.762487499999999]\r  0%|          | 26/9223372036854775807 [1:06:42&lt;182142453150061982:09:04, 71.09s/trial, best loss: 10.762487499999999]\r  0%|          | 27/9223372036854775807 [1:07:41&lt;172918518530815135:17:20, 67.49s/trial, best loss: 10.762487499999999]\r  0%|          | 28/9223372036854775807 [1:08:10&lt;143377833965030149:41:20, 55.96s/trial, best loss: 10.762487499999999]\r  0%|          | 29/9223372036854775807 [1:10:11&lt;193475189461789973:37:04, 75.52s/trial, best loss: 10.762487499999999]\r  0%|          | 30/9223372036854775807 [1:10:31&lt;150845328785940225:08:16, 58.88s/trial, best loss: 10.762487499999999]\r  0%|          | 31/9223372036854775807 [1:10:57&lt;125618291563319041:08:16, 49.03s/trial, best loss: 10.762487499999999]\r  0%|          | 32/9223372036854775807 [1:13:04&lt;185665496756956328:23:28, 72.47s/trial, best loss: 10.762487499999999]\r  0%|          | 33/9223372036854775807 [1:15:44&lt;253093283343333931:14:08, 98.79s/trial, best loss: 10.762487499999999]\r  0%|          | 34/9223372036854775807 [1:15:47&lt;179499328979392921:36:00, 70.06s/trial, best loss: 10.762487499999999]\r  0%|          | 35/9223372036854775807 [1:16:58&lt;179530593112152692:03:12, 70.07s/trial, best loss: 10.762487499999999]\r  0%|          | 36/9223372036854775807 [1:21:10&lt;319596467909638590:00:32, 124.74s/trial, best loss: 10.762487499999999]\r  0%|          | 37/9223372036854775807 [1:21:14&lt;226821108929613696:34:08, 88.53s/trial, best loss: 10.762487499999999] \r  0%|          | 38/9223372036854775807 [1:21:17&lt;161110117141145190:24:00, 62.88s/trial, best loss: 10.762487499999999]\r  0%|          | 39/9223372036854775807 [1:22:52&lt;185896144012428720:21:20, 72.56s/trial, best loss: 10.762487499999999]\r  0%|          | 40/9223372036854775807 [1:23:02&lt;137848892244373303:45:04, 53.80s/trial, best loss: 10.762487499999999]\r  0%|          | 41/9223372036854775807 [1:24:41&lt;172693270625503659:48:16, 67.40s/trial, best loss: 10.762487499999999]\r  0%|          | 42/9223372036854775807 [1:26:54&lt;223242196137651768:53:20, 87.13s/trial, best loss: 10.762487499999999]\r  0%|          | 43/9223372036854775807 [1:28:27&lt;227083239719381929:31:44, 88.63s/trial, best loss: 10.762487499999999]\r  0%|          | 45/9223372036854775807 [1:31:37&lt;232071628299648587:05:36, 90.58s/trial, best loss: 10.762487499999999]\r  0%|          | 46/9223372036854775807 [1:38:29&lt;479572404445012441:18:56, 187.18s/trial, best loss: 10.762487499999999]\r  0%|          | 47/9223372036854775807 [1:38:58&lt;358040951607549041:46:40, 139.75s/trial, best loss: 10.762487499999999]\r  0%|          | 48/9223372036854775807 [1:39:27&lt;272200149310861949:09:20, 106.24s/trial, best loss: 10.762487499999999]\r  0%|          | 49/9223372036854775807 [1:48:27&lt;606046158883107271:06:40, 236.55s/trial, best loss: 10.762487499999999]\r  0%|          | 50/9223372036854775807 [1:50:13&lt;505829017450036374:11:12, 197.43s/trial, best loss: 10.762487499999999]\r  0%|          | 51/9223372036854775807 [1:52:18&lt;450284605466472320:34:08, 175.75s/trial, best loss: 10.762487499999999]\r  0%|          | 52/9223372036854775807 [1:52:28&lt;322152958306058786:08:00, 125.74s/trial, best loss: 10.762487499999999]\r  0%|          | 53/9223372036854775807 [1:52:42&lt;236306135006171809:33:52, 92.23s/trial, best loss: 10.762487499999999] \r  0%|          | 54/9223372036854775807 [1:56:31&lt;341641851250091112:40:32, 133.35s/trial, best loss: 10.762487499999999]\r  0%|          | 55/9223372036854775807 [1:57:02&lt;263028773814110672:12:48, 102.66s/trial, best loss: 10.762487499999999]\r  0%|          | 56/9223372036854775807 [1:59:08&lt;281099178118794044:18:08, 109.72s/trial, best loss: 10.762487499999999]\r  0%|          | 57/9223372036854775807 [1:59:14&lt;201415748433141104:38:24, 78.62s/trial, best loss: 10.762487499999999] \r  0%|          | 58/9223372036854775807 [2:00:41&lt;208043468754668675:58:56, 81.20s/trial, best loss: 10.762487499999999]\r  0%|          | 59/9223372036854775807 [2:05:24&lt;362644146238536999:49:20, 141.54s/trial, best loss: 10.762487499999999]\r  0%|          | 60/9223372036854775807 [2:05:46&lt;270807523961293191:23:44, 105.70s/trial, best loss: 10.762487499999999]\r  0%|          | 61/9223372036854775807 [2:05:47&lt;190365867185325379:07:44, 74.30s/trial, best loss: 10.762487499999999] \r  0%|          | 62/9223372036854775807 [2:08:26&lt;255745234911582253:30:40, 99.82s/trial, best loss: 10.762487499999999]\r  0%|          | 63/9223372036854775807 [2:08:41&lt;190596421255798529:08:16, 74.39s/trial, best loss: 10.762487499999999]\r  0%|          | 64/9223372036854775807 [2:08:58&lt;146527988882035962:18:40, 57.19s/trial, best loss: 10.762487499999999]\r  0%|          | 65/9223372036854775807 [2:11:05&lt;200324184219652441:53:04, 78.19s/trial, best loss: 10.762487499999999]\r  0%|          | 66/9223372036854775807 [2:12:01&lt;182577997180521786:01:36, 71.26s/trial, best loss: 10.762487499999999]\r  0%|          | 67/9223372036854775807 [2:13:37&lt;201701300267349296:55:28, 78.73s/trial, best loss: 10.762487499999999]\r  0%|          | 68/9223372036854775807 [2:15:09&lt;212019451069342861:05:04, 82.75s/trial, best loss: 10.762487499999999]\r  0%|          | 69/9223372036854775807 [2:17:03&lt;236159207168075876:07:28, 92.18s/trial, best loss: 10.762487499999999]\r  0%|          | 70/9223372036854775807 [2:17:27&lt;183937664390135880:49:04, 71.79s/trial, best loss: 10.762487499999999]\r  0%|          | 71/9223372036854775807 [2:17:28&lt;129561570240212354:50:40, 50.57s/trial, best loss: 10.762487499999999]\r  0%|          | 72/9223372036854775807 [2:18:30&lt;138434428592085601:50:56, 54.03s/trial, best loss: 10.762487499999999]\r  0%|          | 73/9223372036854775807 [2:19:58&lt;163887524672763603:37:36, 63.97s/trial, best loss: 10.762487499999999]\r  0%|          | 74/9223372036854775807 [2:22:15&lt;220173922442764178:46:24, 85.94s/trial, best loss: 10.762487499999999]\r  0%|          | 75/9223372036854775807 [2:22:18&lt;156463666627686964:20:16, 61.07s/trial, best loss: 10.762487499999999]\r  0%|          | 76/9223372036854775807 [2:23:11&lt;150340181538111797:28:32, 58.68s/trial, best loss: 10.762487499999999]\r  0%|          | 77/9223372036854775807 [2:24:22&lt;159902290671860303:38:40, 62.41s/trial, best loss: 10.762487499999999]\r  0%|          | 78/9223372036854775807 [2:25:45&lt;175840529917805490:37:52, 68.63s/trial, best loss: 10.762487499999999]\r  0%|          | 79/9223372036854775807 [2:28:56&lt;270101554460032623:30:08, 105.42s/trial, best loss: 10.762487499999999]\r  0%|          | 80/9223372036854775807 [2:29:27&lt;212189201354582880:42:40, 82.82s/trial, best loss: 10.762487499999999] \r  0%|          | 81/9223372036854775807 [2:30:31&lt;197810455336454134:53:52, 77.21s/trial, best loss: 10.762487499999999]\r  0%|          | 82/9223372036854775807 [2:32:29&lt;229296327604178707:20:32, 89.50s/trial, best loss: 10.762487499999999]\r  0%|          | 83/9223372036854775807 [2:32:42&lt;170635341171900780:05:20, 66.60s/trial, best loss: 10.762487499999999]\r  0%|          | 84/9223372036854775807 [2:32:44&lt;121020722529239695:21:36, 47.24s/trial, best loss: 10.762487499999999]\r  0%|          | 85/9223372036854775807 [2:35:21&lt;205563595511261866:40:00, 80.23s/trial, best loss: 10.762487499999999]\r  0%|          | 86/9223372036854775807 [2:35:46&lt;163168939016870188:22:24, 63.69s/trial, best loss: 10.762487499999999]\r  0%|          | 87/9223372036854775807 [2:36:05&lt;128878545389729437:00:48, 50.30s/trial, best loss: 10.762487499999999]\r  0%|          | 88/9223372036854775807 [2:37:22&lt;148732950482525843:54:40, 58.05s/trial, best loss: 10.762487499999999]\r  0%|          | 89/9223372036854775807 [2:37:26&lt;107227465925052115:37:36, 41.85s/trial, best loss: 10.762487499999999]\r  0%|          | 90/9223372036854775807 [2:41:37&lt;268244018381616797:00:48, 104.70s/trial, best loss: 10.762487499999999]\r  0%|          | 91/9223372036854775807 [2:44:59&lt;343253588146276852:37:20, 133.98s/trial, best loss: 10.762487499999999]\r  0%|          | 92/9223372036854775807 [2:47:54&lt;374213018157287851:48:16, 146.06s/trial, best loss: 10.762487499999999]\r  0%|          | 93/9223372036854775807 [2:48:18&lt;280455955173125579:39:44, 109.47s/trial, best loss: 10.762487499999999]\r  0%|          | 94/9223372036854775807 [2:49:03&lt;231105356972067316:37:20, 90.20s/trial, best loss: 10.762487499999999] \r  0%|          | 95/9223372036854775807 [2:50:58&lt;250304967097040604:43:44, 97.70s/trial, best loss: 10.762487499999999]\r  0%|          | 96/9223372036854775807 [2:51:03&lt;179098355014070253:47:44, 69.90s/trial, best loss: 10.762487499999999]\r  0%|          | 97/9223372036854775807 [2:52:02&lt;170808260181083568:21:20, 66.67s/trial, best loss: 10.762487499999999]\r  0%|          | 98/9223372036854775807 [2:52:32&lt;142690289752441833:14:40, 55.69s/trial, best loss: 10.762487499999999]\r  0%|          | 99/9223372036854775807 [2:57:41&lt;336932178937358945:50:56, 131.51s/trial, best loss: 10.762487499999999]\r  0%|          | 100/9223372036854775807 [2:59:22&lt;313612794625655657:48:48, 122.41s/trial, best loss: 10.762487499999999]\r  0%|          | 101/9223372036854775807 [2:59:32&lt;227261271027285906:46:24, 88.70s/trial, best loss: 10.762487499999999] fmin cancelled because of fmin run timeout\n\r  0%|          | 101/9223372036854775807 [3:00:00&lt;273974740036649724:35:12, 106.94s/trial, best loss: 10.762487499999999]\nfmin is cancelled, so new trials will not be launched.\nTotal Trials: 105: 101 succeeded, 0 failed, 4 cancelled.\n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["#### Use MLflow to view the results\nOpen up the Experiment Runs sidebar to see the MLflow runs. Click on Date next to the down arrow to display a menu, and select 'MdAPE' to display the runs sorted by the MdAPE metric. The lowest MdAPE value is ~10%. \n\nMLflow tracks the parameters and performance metrics of each run. Click the External Link icon <img src=\"https://docs.databricks.com/_static/images/external-link.png\"/> at the top of the Experiment Runs sidebar to navigate to the MLflow Runs Table."],"metadata":{}},{"cell_type":"markdown","source":["Now investigate how the hyperparameter choice correlates with MdAPE. Click the \"+\" icon to expand the parent run, then select all runs except the parent, and click \"Compare\". Select the Parallel Coordinates Plot.\n\nThe Parallel Coordinates Plot is useful in understanding the impact of parameters on a metric. You can drag the pink slider bar at the upper right corner of the plot to highlight a subset of MdAPE values and the corresponding parameter values. The plot below highlights the highest MdAPE values:\n\n<img src=\"https://docs.databricks.com/_static/images/mlflow/end-to-end-example/parallel-coordinates-plot.png\"/>\n\nNotice that all of the top performing runs have a low value for reg_lambda and learning_rate. \n\nYou could run another hyperparameter sweep to explore even lower values for these parameters. For simplicity, that step is not included here."],"metadata":{}},{"cell_type":"markdown","source":["You used MLflow to log the model produced by each hyperparameter configuration. The following code finds the best performing run and saves the model to the model registry."],"metadata":{}},{"cell_type":"code","source":["best_run = mlflow.search_runs(order_by=['metrics.MdAPE ASC']).iloc[0]\nprint(f'MdAPE of Best Run: {best_run[\"metrics.MdAPE\"]}')"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">MdAPE of Best Run: 10.762487499999999\n</div>"]}}],"execution_count":17}],"metadata":{"name":"Tune Hyperparameters","notebookId":2750639271612492},"nbformat":4,"nbformat_minor":0}