{"cells":[{"cell_type":"markdown","source":["# Model Explainability\n\n#### Explaining BCI AVM's XGBoost Tree Model with Path-Dependent Feature Perturbation using Tree SHAP\n\n#### About SHAP Values\n\nSHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see papers for details and citations).\n\n\n<p align=\"center\">\n<img width=25% src=\"https://blockchainclimate.org/wp-content/uploads/2020/11/cropped-BCI_Logo_LR-400x333.png\" alt=\"bciAVM\" height=\"300\"/>\n</p>\n\n[![PyPI](https://badge.fury.io/py/bciavm.svg?maxAge=2592000)](https://badge.fury.io/py/bciavm)\n[![PyPI Stats](https://img.shields.io/badge/bciavm-avm-blue)](https://pypistats.org/packages/bciavm)\n\n\nThis notebook contains code to take a `mlflow` registered model and distribute its work with a `Dask` cluster. \n<table>\n    <tr>\n        <td>\n            <img width=25% src=\"https://saturn-public-assets.s3.us-east-2.amazonaws.com/example-resources/dask.png\" width=\"300\">\n        </td>\n    </tr>\n</table>\n\nThe Blockchain & Climate Institute (BCI) is a progressive think tank providing leading expertise in the deployment of emerging technologies for climate and sustainability actions. \n\nAs an international network of scientific and technological experts, BCI is at the forefront of innovative efforts, enabling technology transfers, to create a sustainable and clean global future.\n\n# Automated Valuation Model (AVM) \n\n### About\nAVM is a term for a service that uses mathematical modeling combined with databases of existing properties and transactions to calculate real estate values. \nThe majority of automated valuation models (AVMs) compare the values of similar properties at the same point in time. \nMany appraisers, and even Wall Street institutions, use this type of model to value residential properties. (see [What is an AVM](https://www.investopedia.com/terms/a/automated-valuation-model.asp) Investopedia.com)\n\nFor more detailed info about the AVM, please read the **About** paper found here `resources/2021-BCI-AVM-About.pdf`.\n\n### Valuation Process\n<img src=\"resources/valuation_process.png\" height=\"360\" >\n\n**Key Functionality**\n\n* **Supervised algorithms** \n* **Tree-based & deep learning algorithms** \n* **Feature engineering derived from small clusters of similar properties** \n* **Ensemble (value blending) approaches** \n\n### Set the required AWS Environment Variables\n```shell\nexport ACCESS_KEY=YOURACCESS_KEY\nexport SECRET_KEY=YOURSECRET_KEY\nexport BUCKET_NAME=bci-transition-risk-data\nexport TABLE_DIRECTORY=/dbfs/FileStore/tables/\n```\n\n### Next Steps\nRead more about bciAVM on our [documentation page](https://blockchainclimate.org/thought-leadership/#blog):\n\n### How does it relate to BCI Risk Modeling?\n<img src=\"resources/bci_flowchart_2.png\" height=\"280\" >\n\n\n### Technical & financial support for development provided by:\n<a href=\"https://www.gcode.ai\">\n    <img width=15% src=\"https://staticfiles-img.s3.amazonaws.com/avm/gcode_logo.png\" alt=\"GCODE.ai\"  height=\"25\"/>\n</a>\n\n\n### Install [from PyPI](https://pypi.org/project/bciavm/)\n```shell\npip install bciavm\n```\n\nThis notebook covers the following steps:\n- Import data from your local machine into the Databricks File System (DBFS)\n- Download data from s3\n- Train a machine learning models (or more technically, multiple models in a stacked pipeline) on the dataset\n- Register the model in MLflow"],"metadata":{}},{"cell_type":"code","source":["import bciavm, shap\nimport numpy as np\nimport pandas as pd\nfrom functools import partial\n\nx_path, y_path = '/dbfs/FileStore/tables/avm/X_test.csv', '/dbfs/FileStore/tables/avm/y_test.csv'\npipe_path = '/dbfs/FileStore/artifacts/avm_pipeline_'+str(bciavm.__version__)+'.pkl'\ntarget = 'Price_p'\ndata_index = 'unit_indx'"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["pipeline = bciavm.pipelines.RegressionPipeline.load(pipe_path)\nmodel = pipeline.get_component('XGB Regressor')._component_obj\nmodel"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: XGBRegressor(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type=&#39;gain&#39;, interaction_constraints=&#39;&#39;,\n             learning_rate=0.1040805594257481, max_delta_step=0, max_depth=11,\n             min_child_weight=6, missing=nan, monotone_constraints=&#39;()&#39;,\n             n_estimators=260, n_jobs=4, num_parallel_tree=1, random_state=0,\n             reg_alpha=0.04216890859017286, reg_lambda=0.2541818897259287,\n             scale_pos_weight=1, subsample=1, tree_method=&#39;exact&#39;,\n             validate_parameters=1, verbosity=None)</div>"]}}],"execution_count":3},{"cell_type":"code","source":["#the avm model features\nfeatures = pipeline.get_component('XGB Regressor').input_feature_names\nprint('# of features = ', len(features))\nfeatures"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"># of features =  28\nOut[4]: [&#39;TOTAL_FLOOR_AREA_e&#39;,\n &#39;NUMBER_HEATED_ROOMS_e&#39;,\n &#39;FLOOR_LEVEL_e&#39;,\n &#39;Latitude_m&#39;,\n &#39;Longitude_m&#39;,\n &#39;density_count&#39;,\n &#39;Price_p__mean&#39;,\n &#39;Price_p__std&#39;,\n &#39;Price_p__min&#39;,\n &#39;Price_p__median&#39;,\n &#39;Price_p__max&#39;,\n &#39;TOTAL_FLOOR_AREA_e_minus_mean&#39;,\n &#39;TOTAL_FLOOR_AREA_e_minus_std&#39;,\n &#39;TOTAL_FLOOR_AREA_e_minus_min&#39;,\n &#39;TOTAL_FLOOR_AREA_e_minus_median&#39;,\n &#39;TOTAL_FLOOR_AREA_e_minus_max&#39;,\n &#39;NUMBER_HEATED_ROOMS_e_minus_mean&#39;,\n &#39;NUMBER_HEATED_ROOMS_e_minus_std&#39;,\n &#39;NUMBER_HEATED_ROOMS_e_minus_min&#39;,\n &#39;NUMBER_HEATED_ROOMS_e_minus_median&#39;,\n &#39;NUMBER_HEATED_ROOMS_e_minus_max&#39;,\n &#39;FLOOR_LEVEL_e_minus_mean&#39;,\n &#39;FLOOR_LEVEL_e_minus_std&#39;,\n &#39;FLOOR_LEVEL_e_minus_min&#39;,\n &#39;FLOOR_LEVEL_e_minus_median&#39;,\n &#39;FLOOR_LEVEL_e_minus_max&#39;,\n &#39;agg_cat_B_House_5.0&#39;,\n &#39;agg_cat_S_House_5.0&#39;]</div>"]}}],"execution_count":4},{"cell_type":"code","source":["X = pd.read_csv(x_path)\ny = pd.read_csv(y_path)\n\n#create the explainer\nexplainer = shap.TreeExplainer(model, feature_perturbation=\"tree_path_dependent\")\n\n# define charts\ndef _dependence_plot(features, shap_values, dataset, feature_names, fig_height=15, fig_width=10,  display_features=None, **kwargs):\n    \"\"\" \n    Plots dependence plots of specified features in a grid.\n    \n    features: List[str], List[Tuple[str, str]]\n        Names of features to be plotted. If List[str], then shap \n        values are plotted as a function of feature value, coloured \n        by the value of the feature determined to have the strongest\n        interaction (empirically). If List[Tuple[str, str]], shap\n        interaction values are plotted.\n    display_features: np.ndarray, N x F\n        Same as dataset, but contains human readable values\n        for categorical levels as opposed to numerical values\n    \"\"\"\n    \n    def _set_fonts(fig, ax, fonts=None, set_cbar=False):\n        \"\"\"\n        Sets fonts for axis labels and colobar.\n        \"\"\"\n\n        ax.xaxis.label.set_size(xlabelfontsize)\n        ax.yaxis.label.set_size(ylabelfontsize)\n        ax.tick_params(axis='x', labelsize=xtickfontsize)\n        ax.tick_params(axis='y', labelsize=ytickfontsize)\n        if set_cbar:\n            fig.axes[-1].tick_params(labelsize=cbartickfontsize)\n            fig.axes[-1].tick_params(labelrotation=cbartickrotation)\n            fig.axes[-1].yaxis.label.set_size(cbarlabelfontsize)\n\n    # parse plotting args\n    figsize = kwargs.get(\"figsize\", (fig_height, fig_width))\n    nrows = kwargs.get('nrows', len(features))\n    ncols = kwargs.get('ncols', 1)\n    xlabelfontsize = kwargs.get('xlabelfontsize', 14)\n    xtickfontsize = kwargs.get('xtickfontsize', 11)\n    ylabelfontsize = kwargs.get('ylabelfontsize', 14)\n    ytickfontsize = kwargs.get('ytickfontsize', 11)\n    cbartickfontsize = kwargs.get('cbartickfontsize', 14)\n    cbartickrotation = kwargs.get('cbartickrotation', 45)\n    cbarlabelfontsize = kwargs.get('cbarlabelfontsize', 14)\n    rotation_orig = kwargs.get('xticklabelrotation', 25)\n    alpha = kwargs.get(\"alpha\", 1)\n    x_jitter_orig = kwargs.get(\"x_jitter\", 0.8)\n    grouped_features = list(zip_longest(*[iter(features)] * ncols))\n    \n    fig, axes = plt.subplots(nrows, ncols,  figsize=figsize)\n    if nrows == len(features):\n        axes = list(zip_longest(*[iter(axes)] * 1))\n\n\n    for i, (row, group) in enumerate(zip(axes, grouped_features), start=1):\n        # plot each feature or interaction in a subplot\n        for ax, feature in zip(row, group):\n            # set x-axis ticks and labels and x-jitter for categorical variables\n            if not feature:\n                continue\n            if isinstance(feature, list) or isinstance(feature, tuple):\n                feature_index = feature_names.index(feature[0])\n            else:\n                feature_index = feature_names.index(feature)\n\n            x_jitter = 0\n            \n            shap.dependence_plot(feature, \n                                 shap_values,\n                                 dataset,\n                                 feature_names=feature_names,\n                                 display_features=display_features,\n                                 interaction_index='auto',\n                                 ax=ax,\n                                 show=False,\n                                 x_jitter=x_jitter,\n                                 alpha=alpha\n                                )\n            if i!= nrows:\n                ax.tick_params('x', labelrotation=rotation_orig)\n            _set_fonts(fig, ax, set_cbar=True)\n    \n\ndef plot_decomposition(feature_pair, shap_interaction_vals, features, feat_names, display_features=None, **kwargs):\n    \"\"\"\n    Given a list containing two feature names (`feature_pair`), an n_instances x n_features x n_features tensor \n    of shap interaction values (`shap_interaction_vals`), an n_instances x n_features (`features`) tensor of \n    feature values and a list of feature names (which assigns a name to each column of `features`), this function \n    plots:\n        - left: shap values for feature_pair[0] coloured by the value of feature_pair[1]\n        - middle: shap values for feature_pair[0] after subtracting the interaction with feature_pair[1]\n        - right: the interaction values between feature_pair[0] and feature_pair[1], which are subtracted \n        from the left plot to get the middle plot\n        \n    NB: `display_features` is the same shape as `features` but should contain the raw categories for categorical \n    variables so that the colorbar can be discretised and the category names displayed alongside the colorbar.\n    \"\"\"\n    \n    def _set_fonts(fig, ax, fonts=None, set_cbar=False):\n        \"\"\"\n        Sets fonts for axis labels and colobar.\n        \"\"\"\n\n        ax.xaxis.label.set_size(xlabelfontsize)\n        ax.yaxis.label.set_size(ylabelfontsize)\n        ax.tick_params(axis='x', labelsize=xtickfontsize)\n        ax.tick_params(axis='y', labelsize=ytickfontsize)\n        if set_cbar:\n            fig.axes[-1].tick_params(labelsize=cbartickfontsize)\n            fig.axes[-1].yaxis.label.set_size(cbarlabelfontsize)\n\n    # parse plotting args\n    xlabelfontsize = kwargs.get('xlabelfontsize', 21)\n    ylabelfontsize = kwargs.get('ylabelfontsize', 16)\n    cbartickfontsize = kwargs.get('cbartickfontsize', 16)\n    cbarlabelfontsize = kwargs.get('cbarlabelfontsize', 21)\n    xtickfontsize = kwargs.get('xtickfontsize', 20)\n    ytickfontsize = kwargs.get('ytickfontsize', 16)\n    alpha = kwargs.get('alpha', 0.7)\n    figsize = kwargs.get('figsize', (44, 16))\n    ncols = kwargs.get('ncols', 3)\n    nrows = kwargs.get('nrows', 1)\n    \n    # compute shap values and shap values without interaction\n    feat1_idx = feat_names.index(feature_pair[0])\n    feat2_idx = feat_names.index(feature_pair[1])\n    \n    # shap values\n    shap_vals = shap_interaction_vals.sum(axis=2)\n    \n    # shap values for feat1, all samples\n    shap_val_ind1 = shap_interaction_vals[..., feat1_idx].sum(axis=1)\n    \n    # shap values for (feat1, feat2) interaction \n    shap_int_ind1_ind2 = shap_interaction_vals[:, feat2_idx, feat1_idx]\n    \n    # subtract effect of feat2\n    shap_val_minus_ind2 = shap_val_ind1 - shap_int_ind1_ind2\n    shap_val_minus_ind2 = shap_val_minus_ind2[:, None]\n\n    # create plot\n    fig, (ax1, ax2, ax3) = plt.subplots(nrows, ncols, figsize=figsize)\n\n    # plot the shap values including the interaction\n    shap.dependence_plot(feature_pair[0],\n                         shap_vals,\n                         features,\n                         display_features = display_features,\n                         feature_names=feat_names,\n                         interaction_index=feature_pair[1],\n                         alpha=alpha,\n                         ax=ax1,\n                         show=False)\n    _set_fonts(fig, ax1, set_cbar=True)\n\n    # plot the shap values excluding the interaction\n    shap.dependence_plot(0,\n                         shap_val_minus_ind2,\n                         features[:, feat1_idx][:, None],\n                         feature_names=[feature_pair[0]],\n                         interaction_index=None,\n                         alpha=alpha,\n                         ax=ax2,\n                         show=False,\n                         )\n    ax2.set_ylabel(f' Shap value for  {feature_pair[0]} \\n wo {feature_pair[1]} interaction')\n    _set_fonts(fig, ax2)\n    \n    # plot the interaction value\n    shap.dependence_plot(feature_pair,\n                         shap_interaction_vals,\n                         features,\n                         feature_names=feat_names,\n                         display_features=display_features,\n                         interaction_index='auto',\n                         alpha=alpha,\n                         ax=ax3,\n                         show=False,\n                        )\n    _set_fonts(fig, ax3, set_cbar=True)\n    \n\n\n\ndef plot_decomposition(feature_pair, shap_interaction_vals, features, feat_names, display_features=None, **kwargs):\n    \"\"\"\n    Given a list containing two feature names (`feature_pair`), an n_instances x n_features x n_features tensor \n    of shap interaction values (`shap_interaction_vals`), an n_instances x n_features (`features`) tensor of \n    feature values and a list of feature names (which assigns a name to each column of `features`), this function \n    plots:\n        - left: shap values for feature_pair[0] coloured by the value of feature_pair[1]\n        - middle: shap values for feature_pair[0] after subtracting the interaction with feature_pair[1]\n        - right: the interaction values between feature_pair[0] and feature_pair[1], which are subtracted \n        from the left plot to get the middle plot\n        \n    NB: `display_features` is the same shape as `features` but should contain the raw categories for categorical \n    variables so that the colorbar can be discretised and the category names displayed alongside the colorbar.\n    \"\"\"\n    \n    def _set_fonts(fig, ax, fonts=None, set_cbar=False):\n        \"\"\"\n        Sets fonts for axis labels and colobar.\n        \"\"\"\n\n        ax.xaxis.label.set_size(xlabelfontsize)\n        ax.yaxis.label.set_size(ylabelfontsize)\n        ax.tick_params(axis='x', labelsize=xtickfontsize)\n        ax.tick_params(axis='y', labelsize=ytickfontsize)\n        if set_cbar:\n            fig.axes[-1].tick_params(labelsize=cbartickfontsize)\n            fig.axes[-1].yaxis.label.set_size(cbarlabelfontsize)\n\n    # parse plotting args\n    xlabelfontsize = kwargs.get('xlabelfontsize', 21)\n    ylabelfontsize = kwargs.get('ylabelfontsize', 16)\n    cbartickfontsize = kwargs.get('cbartickfontsize', 16)\n    cbarlabelfontsize = kwargs.get('cbarlabelfontsize', 21)\n    xtickfontsize = kwargs.get('xtickfontsize', 20)\n    ytickfontsize = kwargs.get('ytickfontsize', 16)\n    alpha = kwargs.get('alpha', 0.7)\n    figsize = kwargs.get('figsize', (44, 16))\n    ncols = kwargs.get('ncols', 3)\n    nrows = kwargs.get('nrows', 1)\n    # compute shap values and shap values without interaction\n    feat1_idx = feat_names.index(feature_pair[0])\n    feat2_idx = feat_names.index(feature_pair[1])\n    # shap values\n    shap_vals = shap_interaction_vals.sum(axis=2)\n    # shap values for feat1, all samples\n    shap_val_ind1 = shap_interaction_vals[..., feat1_idx].sum(axis=1)\n    # shap values for (feat1, feat2) interaction \n    shap_int_ind1_ind2 = shap_interaction_vals[:, feat2_idx, feat1_idx]\n    # subtract effect of feat2\n    shap_val_minus_ind2 = shap_val_ind1 - shap_int_ind1_ind2\n    shap_val_minus_ind2 = shap_val_minus_ind2[:, None]\n\n    # create plot\n\n    fig, (ax1, ax2, ax3) = plt.subplots(nrows, ncols, figsize=figsize)\n\n    # plot the shap values including the interaction\n    shap.dependence_plot(feature_pair[0],\n                         shap_vals,\n                         features,\n                         display_features = display_features,\n                         feature_names=feat_names,\n                         interaction_index=feature_pair[1],\n                         alpha=alpha,\n                         ax=ax1,\n                         show=False)\n    _set_fonts(fig, ax1, set_cbar=True)\n\n    # plot the shap values excluding the interaction\n    shap.dependence_plot(0,\n                         shap_val_minus_ind2,\n                         features[:, feat1_idx][:, None],\n                         feature_names=[feature_pair[0]],\n                         interaction_index=None,\n                         alpha=alpha,\n                         ax=ax2,\n                         show=False,\n                         )\n    ax2.set_ylabel(f' Shap value for  {feature_pair[0]} \\n wo {feature_pair[1]} interaction')\n    _set_fonts(fig, ax2)\n    \n    # plot the interaction value\n    shap.dependence_plot(feature_pair,\n                         shap_interaction_vals,\n                         features,\n                         feature_names=feat_names,\n                         display_features=display_features,\n                         interaction_index='auto',\n                         alpha=alpha,\n                         ax=ax3,\n                         show=False)\n    \n    _set_fonts(fig, ax3, set_cbar=True)\n    \n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["preprocessor = pipeline.get_component(\"Preprocess Transformer\")\nimputer = pipeline.get_component('Imputer')\nohe = pipeline.get_component('One Hot Encoder')"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["X_processed = X.copy()\nX_t = preprocessor.transform(X_processed)\nX_t = imputer.transform(X_t)\nX_t = ohe.transform(X_t)\nX_t = X_t[features]\nX_t"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: </div>"]}},{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TOTAL_FLOOR_AREA_e</th>\n      <th>NUMBER_HEATED_ROOMS_e</th>\n      <th>FLOOR_LEVEL_e</th>\n      <th>Latitude_m</th>\n      <th>Longitude_m</th>\n      <th>density_count</th>\n      <th>Price_p__mean</th>\n      <th>Price_p__std</th>\n      <th>Price_p__min</th>\n      <th>Price_p__median</th>\n      <th>Price_p__max</th>\n      <th>TOTAL_FLOOR_AREA_e_minus_mean</th>\n      <th>TOTAL_FLOOR_AREA_e_minus_std</th>\n      <th>TOTAL_FLOOR_AREA_e_minus_min</th>\n      <th>TOTAL_FLOOR_AREA_e_minus_median</th>\n      <th>TOTAL_FLOOR_AREA_e_minus_max</th>\n      <th>NUMBER_HEATED_ROOMS_e_minus_mean</th>\n      <th>NUMBER_HEATED_ROOMS_e_minus_std</th>\n      <th>NUMBER_HEATED_ROOMS_e_minus_min</th>\n      <th>NUMBER_HEATED_ROOMS_e_minus_median</th>\n      <th>NUMBER_HEATED_ROOMS_e_minus_max</th>\n      <th>FLOOR_LEVEL_e_minus_mean</th>\n      <th>FLOOR_LEVEL_e_minus_std</th>\n      <th>FLOOR_LEVEL_e_minus_min</th>\n      <th>FLOOR_LEVEL_e_minus_median</th>\n      <th>FLOOR_LEVEL_e_minus_max</th>\n      <th>agg_cat_B_House_5.0</th>\n      <th>agg_cat_S_House_5.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>75.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>51.455688</td>\n      <td>-0.161738</td>\n      <td>6.0</td>\n      <td>212000.0</td>\n      <td>40305.086528</td>\n      <td>155750.0</td>\n      <td>205000.0</td>\n      <td>280000.0</td>\n      <td>-2.043333</td>\n      <td>66.740622</td>\n      <td>15.00</td>\n      <td>0.000</td>\n      <td>-25.0</td>\n      <td>0.00</td>\n      <td>3.500000</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>103.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>52.912711</td>\n      <td>-1.144708</td>\n      <td>2.0</td>\n      <td>320000.0</td>\n      <td>21213.203436</td>\n      <td>305000.0</td>\n      <td>320000.0</td>\n      <td>335000.0</td>\n      <td>0.500000</td>\n      <td>92.393398</td>\n      <td>8.00</td>\n      <td>0.500</td>\n      <td>-7.0</td>\n      <td>0.50</td>\n      <td>5.292893</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>77.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>51.425884</td>\n      <td>-0.423289</td>\n      <td>8.0</td>\n      <td>429000.0</td>\n      <td>126471.905406</td>\n      <td>325000.0</td>\n      <td>387500.0</td>\n      <td>705000.0</td>\n      <td>-12.506250</td>\n      <td>53.299150</td>\n      <td>11.80</td>\n      <td>-5.000</td>\n      <td>-67.0</td>\n      <td>0.50</td>\n      <td>3.585786</td>\n      <td>3.0</td>\n      <td>0.5</td>\n      <td>-2.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>90.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>53.838379</td>\n      <td>-1.547717</td>\n      <td>6.0</td>\n      <td>212000.0</td>\n      <td>40305.086528</td>\n      <td>155750.0</td>\n      <td>205000.0</td>\n      <td>280000.0</td>\n      <td>-2.043333</td>\n      <td>66.740622</td>\n      <td>15.00</td>\n      <td>0.000</td>\n      <td>-25.0</td>\n      <td>0.00</td>\n      <td>3.500000</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>75.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>52.584828</td>\n      <td>-0.225782</td>\n      <td>5.0</td>\n      <td>69200.0</td>\n      <td>24724.987361</td>\n      <td>42500.0</td>\n      <td>65000.0</td>\n      <td>110000.0</td>\n      <td>28.200000</td>\n      <td>58.531849</td>\n      <td>38.00</td>\n      <td>38.000</td>\n      <td>0.0</td>\n      <td>0.80</td>\n      <td>2.552786</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>-0.4</td>\n      <td>-0.140175</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>-2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>109809</th>\n      <td>100.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>54.888152</td>\n      <td>-2.941256</td>\n      <td>6.0</td>\n      <td>212000.0</td>\n      <td>40305.086528</td>\n      <td>155750.0</td>\n      <td>205000.0</td>\n      <td>280000.0</td>\n      <td>-2.043333</td>\n      <td>66.740622</td>\n      <td>15.00</td>\n      <td>0.000</td>\n      <td>-25.0</td>\n      <td>0.00</td>\n      <td>3.500000</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>109810</th>\n      <td>229.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>54.134593</td>\n      <td>-0.808554</td>\n      <td>6.0</td>\n      <td>212000.0</td>\n      <td>40305.086528</td>\n      <td>155750.0</td>\n      <td>205000.0</td>\n      <td>280000.0</td>\n      <td>-2.043333</td>\n      <td>66.740622</td>\n      <td>15.00</td>\n      <td>0.000</td>\n      <td>-25.0</td>\n      <td>0.00</td>\n      <td>3.500000</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>109811</th>\n      <td>66.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>50.206130</td>\n      <td>-5.385605</td>\n      <td>6.0</td>\n      <td>212000.0</td>\n      <td>40305.086528</td>\n      <td>155750.0</td>\n      <td>205000.0</td>\n      <td>280000.0</td>\n      <td>-2.043333</td>\n      <td>66.740622</td>\n      <td>15.00</td>\n      <td>0.000</td>\n      <td>-25.0</td>\n      <td>0.00</td>\n      <td>3.500000</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>109812</th>\n      <td>191.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>53.381240</td>\n      <td>-2.318664</td>\n      <td>2.0</td>\n      <td>347500.0</td>\n      <td>102530.483272</td>\n      <td>275000.0</td>\n      <td>347500.0</td>\n      <td>420000.0</td>\n      <td>53.225000</td>\n      <td>115.728483</td>\n      <td>106.45</td>\n      <td>53.225</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>5.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>109813</th>\n      <td>156.0</td>\n      <td>8.0</td>\n      <td>0.0</td>\n      <td>53.040690</td>\n      <td>-3.002595</td>\n      <td>20.0</td>\n      <td>104325.0</td>\n      <td>19570.570683</td>\n      <td>71000.0</td>\n      <td>102000.0</td>\n      <td>147000.0</td>\n      <td>68.177600</td>\n      <td>132.588450</td>\n      <td>98.00</td>\n      <td>75.000</td>\n      <td>0.0</td>\n      <td>3.25</td>\n      <td>6.748685</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>109814 rows × 28 columns</p>\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["#Use a small sample bc SHAP is slow\nXs = X_t.sample(100)\n\n# #calculate the shap values\nshap_values = explainer.shap_values(Xs.values, check_additivity=False)\n\n# #calculate the interaction values\nshap_interaction_values = explainer.shap_interaction_values(Xs.values)\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["### Figure 1: Features that lower the AVM valuation\n- A lower `TOTAL_FLOOR_AREA_e` feature is generally indicative of a lower AVM valuation\n- A lower `Price_p__median` feature is generally indicative of a lower AVM valuation\n- A lower `Price_p__mean` is generally indicative of a lower AVM value\n- A higher `Latitude_m` is generally indicative of a lower AVM value"],"metadata":{}},{"cell_type":"code","source":["shap.summary_plot(shap_values, Xs, features)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"image/png":"/plots/840b639a-2a92-4576-9ec8-36ab0ca0ce61.png"}}],"execution_count":10},{"cell_type":"markdown","source":["### Figure 2: Features that increase the AVM valuation\n- The inverse of the above is also true."],"metadata":{}},{"cell_type":"code","source":["shap.summary_plot(shap_values, Xs, features)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"image/png":"/plots/51ddec7b-c8ef-459f-899c-dc8f4f2cecb4.png"}}],"execution_count":12},{"cell_type":"markdown","source":["### Figure 3: Interesting Partial Dependence Plots\n\nWhile the above SHAP summary plots give a general overview of each feature, the below SHAP dependence plots show how the model output varies by feauture value. \n\nIn the Summary Plots, note that every dot is a `unit_indx` valuation, and the vertical dispersion at a single feature value results from interaction effects in the model. The feature used for coloring is automatically chosen to highlight what might be driving these interactions. Later we will see how to check that the interaction is really in the model with SHAP interaction values. Note that the row of a SHAP summary plot results from projecting the points of a SHAP dependence plot onto the y-axis, then recoloring by the feature itself.\n\nBelow we give the SHAP dependence plot for each of the chosen features, revealing interesting but expected trends. Keep in mind the calibration of some of these values can be different than a real world effect, so it is wise to be careful drawing concrete conclusions.\n\n- **(top left)** \n\n- **(top right)** \n\n- **(bottom left)** \n\n- **(bottom right)**"],"metadata":{}},{"cell_type":"code","source":["from itertools import product, zip_longest\nimport matplotlib.pyplot as plt\n\nplot_dependence = partial(\n    _dependence_plot, \n    feature_names=features,\n    category_map={},\n)\n\nplot_dependence(\n    [ \n     'TOTAL_FLOOR_AREA_e',\n     'NUMBER_HEATED_ROOMS_e', \n     'FLOOR_LEVEL_e', \n     'Price_p__median'\n    ], \n    shap_values, \n    Xs, \n    display_features = Xs,  \n    rotation=33,\n    figsize=(40, 20), \n    alpha=1, \n    x_jitter=0.5,\n    nrows=2,\n    ncols=2,\n    xlabelfontsize=24,\n    xtickfontsize=20,\n    xticklabelrotation=0,\n    ylabelfontsize=24,\n    ytickfontsize=21,\n    cbarlabelfontsize=22,\n    cbartickfontsize=20,\n    cbartickrotation=0\n)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"image/png":"/plots/0edc4b8e-9d3e-40cf-9a73-b04a8986da97.png"}}],"execution_count":14},{"cell_type":"markdown","source":["#### Figure 4: Percent Error By Price Range\n\nWe can calculate and graph the % Error by Price Range as follows:"],"metadata":{}},{"cell_type":"code","source":["\ndef _plot_accuracy_by_price_range(y_test, y_stacked, units):\n    model_name='avm'\n\n    df_acc = pd.DataFrame({'y_test': y_test, 'preds': y_stacked, 'unit_indx': units}, columns=['y_test', 'preds', 'unit_indx'])\n    df_acc['error'] = abs((df_acc['preds'] - df_acc['y_test']) / df_acc['y_test'])\n    df_acc['percent_error'] = (df_acc['preds'] - df_acc['y_test']) / df_acc['y_test']\n\n    ctt = []\n    try: \n        if len(df_acc[df_acc['y_test']<=50000]) > 5: ctt.append('df_0')\n    except: pass\n    try: \n        if len(df_acc[(df_acc['y_test']>=50000) & (df_acc['y_test']<=100000)]) > 5: ctt.append('df_5')\n    except: pass\n    try: \n        if len(df_acc[(df_acc['y_test']>=100000) & (df_acc['y_test']<=150000)]) > 5: ctt.append('df_10')\n    except: pass\n    try: \n        if len(df_acc[(df_acc['y_test']>=150000) & (df_acc['y_test']<=200000)]) > 5: ctt.append('df_15')\n    except: pass\n    try: \n        if len(df_acc[(df_acc['y_test']>=200000) & (df_acc['y_test']<=250000)]) > 5: ctt.append('df_20')\n    except: pass\n    try: \n        if len(df_acc[(df_acc['y_test']>=250000) & (df_acc['y_test']<=300000)]) > 5: ctt.append('df_25')\n    except: pass\n    try: \n        if len(df_acc[(df_acc['y_test']>=300000) & (df_acc['y_test']<=350000)]) > 5: ctt.append('df_30')\n    except: pass\n    try: \n        if len(df_acc[(df_acc['y_test']>=350000) & (df_acc['y_test']<=400000)]) > 5: ctt.append('df_35')\n    except: pass\n    try: \n        if len(df_acc[df_acc['y_test']>=400000]) > 5: ctt.append('df_40')\n    except: pass\n\n    def troubleshoot(ctt, df_acc):\n\n        acc_2 = len(ctt)*[0]\n        acc_5 = len(ctt)*[0]\n        acc_10 = len(ctt)*[0]\n        acc_20 = len(ctt)*[0]\n\n        x = 0\n        if 'df_0' in ctt:\n            try:\n                df_0 = df_acc[df_acc['y_test']<=50000]\n                acc_2[x] = len(df_0[df_0['error']<0.02])/len(df_0)\n                acc_5[x] = len(df_0[df_0['error']<0.05])/len(df_0)\n                acc_10[x] = len(df_0[df_0['error']<0.1])/len(df_0)\n                acc_20[x] = len(df_0[df_0['error']<0.2])/len(df_0)\n                x += 1\n            except: \n                ctt.pop('df_0')\n                troubleshoot(ctt, df_acc)\n\n        if 'df_5' in ctt:\n            try:\n                df_5 = df_acc[(df_acc['y_test']>=50000) & (df_acc['y_test']<=100000)]\n                acc_2[x] = len(df_5[df_5['error']<0.02])/len(df_5)\n                acc_5[x] = len(df_5[df_5['error']<0.05])/len(df_5)\n                acc_10[x] = len(df_5[df_5['error']<0.10])/len(df_5)\n                acc_20[x] = len(df_5[df_5['error']<0.2])/len(df_5)\n                x += 1\n            except: \n                ctt.pop('df_5')\n                troubleshoot(ctt, df_acc)\n\n        if 'df_10' in ctt:\n            try:\n                df_10 = df_acc[(df_acc['y_test']>=100000) & (df_acc['y_test']<=150000)]\n                acc_2[x] = len(df_10[df_10['error']<0.02])/len(df_10)\n                acc_5[x] = len(df_10[df_10['error']<0.05])/len(df_10)\n                acc_10[x] = len(df_10[df_10['error']<0.10])/len(df_10)\n                acc_20[x] = len(df_10[df_10['error']<0.2])/len(df_10)\n                x += 1\n            except: \n                ctt.pop('df_10')\n                troubleshoot(ctt, df_acc)\n\n        if 'df_10' in ctt:\n            try:\n                df_15 = df_acc[(df_acc['y_test']>=150000) & (df_acc['y_test']<=200000)]\n                acc_2[x] = len(df_15[df_15['error']<0.02])/len(df_15)\n                acc_5[x] = len(df_15[df_15['error']<0.05])/len(df_15)\n                acc_10[x] = len(df_15[df_15['error']<0.1])/len(df_15)\n                acc_20[x] = len(df_15[df_15['error']<0.2])/len(df_15)\n                x += 1\n            except: \n                ctt.pop('df_10')\n                troubleshoot(ctt, df_acc)\n\n        if 'df_20' in ctt:\n            try:\n                df_20 = df_acc[(df_acc['y_test']>=200000) & (df_acc['y_test']<=250000)]\n                acc_2[x] = len(df_20[df_20['error']<0.02])/len(df_20)\n                acc_5[x] = len(df_20[df_20['error']<0.05])/len(df_20)\n                acc_10[x] = len(df_20[df_20['error']<0.1])/len(df_20)\n                acc_20[x] = len(df_20[df_20['error']<0.2])/len(df_20)\n                x += 1\n            except: \n                ctt.pop('df_20')\n                troubleshoot(ctt, df_acc)\n\n        if 'df_25' in ctt:\n            try:\n                df_25 = df_acc[(df_acc['y_test']>=250000) & (df_acc['y_test']<=300000)]\n                acc_2[x] = len(df_25[df_25['error']<0.02])/len(df_25)\n                acc_5[x] = len(df_25[df_25['error']<0.05])/len(df_25)\n                acc_10[x] = len(df_25[df_25['error']<0.1])/len(df_25)\n                acc_20[x] = len(df_25[df_25['error']<0.2])/len(df_25)\n                x += 1\n            except: \n                ctt.pop('df_25')\n                troubleshoot(ctt, df_acc)\n\n        if 'df_30' in ctt:\n            try:\n                df_30 = df_acc[(df_acc['y_test']>=300000) & (df_acc['y_test']<=350000)]\n                acc_2[x] = len(df_30[df_30['error']<0.02])/len(df_30)\n                acc_5[x] = len(df_30[df_30['error']<0.05])/len(df_30)\n                acc_10[x] = len(df_30[df_30['error']<0.1])/len(df_30)\n                acc_20[x] = len(df_30[df_30['error']<0.2])/len(df_30)\n                x += 1\n            except: \n                ctt.pop('df_30')\n                troubleshoot(ctt, df_acc)\n\n        if 'df_35' in ctt:\n            try:\n                df_35 = df_acc[(df_acc['y_test']>=350000) & (df_acc['y_test']<=400000)]\n                acc_2[x] = len(df_35[df_35['error']<0.02])/len(df_35)\n                acc_5[x] = len(df_35[df_35['error']<0.05])/len(df_35)\n                acc_10[x] = len(df_35[df_35['error']<0.1])/len(df_35)\n                acc_20[x] = len(df_35[df_35['error']<0.2])/len(df_35)\n                x += 1\n            except: \n                ctt.pop('df_35')\n                troubleshoot(ctt, df_acc)\n\n        if 'df_40' in ctt:\n            try:\n                df_40 = df_acc[df_acc['y_test']>=400000]\n                acc_2[x] = len(df_40[df_40['error']<0.02])/len(df_40)\n                acc_5[x] = len(df_40[df_40['error']<0.05])/len(df_40)\n                acc_10[x] = len(df_40[df_40['error']<0.1])/len(df_40)\n                acc_20[x] = len(df_40[df_40['error']<0.2])/len(df_40)\n                x += 1\n            except: \n                ctt.pop('df_40')\n                troubleshoot(ctt, df_acc)\n\n        return acc_2, acc_5, acc_10, acc_20, ctt\n\n    acc_2, acc_5, acc_10, acc_20, ctt = troubleshoot(ctt, df_acc)\n\n    acc = pd.DataFrame(\n        {'acc_2': acc_2,\n         'acc_5': acc_5,\n         'acc_10': acc_10,\n         'acc_20': acc_20,\n        })\n\n    a = []\n    if 'df_0' in ctt:\n        a.append('<50k')\n    if 'df_5' in ctt:\n        a.append('50k-100k')\n    if 'df_10' in ctt:\n        a.append('100k-150k')\n    if 'df_15' in ctt:\n        a.append('150k-200k')\n    if 'df_20' in ctt:\n        a.append('200k-250k')\n    if 'df_25' in ctt:\n        a.append('250k-300k')\n    if 'df_30' in ctt:\n        a.append('300k-350k')\n    if 'df_35' in ctt:\n        a.append('350k-400k')\n    if 'df_40' in ctt:\n        a.append('>400k')\n\n    acc['value bands'] = a\n\n    plt.style.use('ggplot')\n    fig, ax = plt.subplots(figsize = (20,10))\n    #sns.catplot(x=\"acc_5\", y=\"percentile\", data=acc,ax=ax)\n    ax.scatter(acc['acc_2'], acc['value bands'],label='err +-2%', s=80)\n    ax.scatter(acc['acc_5'], acc['value bands'],label='err +-5%', s=80)\n    ax.scatter(acc['acc_10'], acc['value bands'],label='err +-10%', s=80)\n    ax.scatter(acc['acc_20'], acc['value bands'],label='err +-20%', s=80)\n\n    for i in range(0, len(ctt)):\n        plt.plot([acc['acc_2'][i], acc['acc_10'][i]], [[i]*5,[i]*5], 'grey')\n        plt.plot([acc['acc_5'][i], acc['acc_20'][i]], [[i]*5,[i]*5], 'grey')\n\n    plt.legend(fontsize=15)\n    ax.set_xlim(0, 1)\n    ax.set_xticklabels([0,20,40,60,80,100], rotation=0, fontsize=15)\n    ax.set_yticklabels(acc['value bands'], rotation=0, fontsize=15)\n    ax.set_xlabel('Percent of valuations within +-5%, +-20%')\n    ax.set_ylabel('Price range of properties £ Pounds')\n    plt.title('Accuracy by Value ' + model_name,fontsize=20)\n    plt.show()\n    imname = 'Accuracy by Price Range.png'\n    plt.savefig(imname)\n    print('end plot_by_price_range...')\n    \n    X_misvalued_abs = df_acc[df_acc['error']< 0.2]\n    X_misvalued_low = df_acc[df_acc['percent_error'] < -0.2]\n    X_misvalued_high = df_acc[df_acc['percent_error'] > 0.2]\n    return X_misvalued_low, X_misvalued_high, X_misvalued_abs, df_acc\n    "],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["Xx = X\ny_true = y['Price_p'].values"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["y_predicted = pipeline.predict(Xx)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["#sanity check the chart below\n\nscores = pipeline.score(X, \n                        y['Price_p'], \n                        objectives=['MAPE',\n                                 'MdAPE',\n                                 'ExpVariance',\n                                 'MaxError',\n                                 'MedianAE',\n                                 'MSE',\n                                 'MAE',\n                                 'R2',\n                                 'Root Mean Squared Error'])\nscores"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[16]: OrderedDict([(&#39;MAPE&#39;, 15.426955429844297),\n             (&#39;MdAPE&#39;, 9.454798377403845),\n             (&#39;ExpVariance&#39;, 0.892766399577722),\n             (&#39;MaxError&#39;, 920230.5625),\n             (&#39;MedianAE&#39;, 20012.52734375),\n             (&#39;MSE&#39;, 3182616007.2215877),\n             (&#39;MAE&#39;, 33918.25863464657),\n             (&#39;R2&#39;, 0.8927646965814696),\n             (&#39;Root Mean Squared Error&#39;, 56414.67900486174)])</div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["From the plot below we can see that in general:\n- Approx 15% of valuations have an absolute error < +-2%\n- Approx 35% of valuations have an absolute error < +-5%\n- Over 55% of valuations have an absolute error < +-10%\n- Approx 75% of valuations have an absolute error < +-20%"],"metadata":{}},{"cell_type":"code","source":["#plot and get high error units\nX_misvalued_low, X_misvalued_high, X_misvalued_abs, df_acc = _plot_accuracy_by_price_range(y_true, y_predicted, Xx['unit_indx'].values)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"image/png":"/plots/2e7fc9f1-55ce-4824-9f73-152fee0f089e.png"}},{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">end plot_by_price_range...\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["#find high errors below the true value\nX_misvalued_low_units = list(X_misvalued_low['unit_indx'].values)\nX_misvalued_low = Xx[Xx['unit_indx'].isin(X_misvalued_low_units)]\n\n#find high errors above the true value\nX_misvalued_high_units = list(X_misvalued_high['unit_indx'].values)\nX_misvalued_high = Xx[Xx['unit_indx'].isin(X_misvalued_high_units)]\n\n#find high errors above the true value\nX_misvalued_abs_units = list(X_misvalued_abs['unit_indx'].values)\nX_misvalued_abs = Xx[Xx['unit_indx'].isin(X_misvalued_abs_units)]\n\ncolumns = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31']\nX_misvalued_low_t = preprocessor.transform(X_misvalued_low)\nX_misvalued_low_t = imputer.transform(X_misvalued_low_t)\nX_misvalued_low_t = ohe.transform(X_misvalued_low_t)\nX_misvalued_low_t = X_misvalued_low_t[features]\n\nX_misvalued_high_t = preprocessor.transform(X_misvalued_high)\nX_misvalued_high_t = imputer.transform(X_misvalued_high_t)\nX_misvalued_high_t = ohe.transform(X_misvalued_high_t)\nX_misvalued_high_t = X_misvalued_high_t[features]\n\nX_misvalued_abs_t = preprocessor.transform(X_misvalued_abs)\nX_misvalued_abs_t = imputer.transform(X_misvalued_abs_t)\nX_misvalued_abs_t = ohe.transform(X_misvalued_abs_t)\nX_misvalued_abs_t = X_misvalued_abs_t[features]\n\nX_misvalued_abs_t = X_misvalued_abs_t.sample(500)\nX_misvalued_high_t = X_misvalued_high_t.sample(500)\nX_misvalued_low_t = X_misvalued_low_t.sample(500)\n\n#calc low & high error SHAP values\nshap_values_misvalued_low = explainer.shap_values(X_misvalued_low_t.values, check_additivity=False)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["shap_values_misvalued_high = explainer.shap_values(X_misvalued_high_t.values, check_additivity=False)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["shap_values_misvalued_abs = explainer.shap_values(X_misvalued_abs_t.values, check_additivity=False)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["### Figure 5: Understanding High Error Valuations\n\nBut what about valuations with over 20% error? Is there any way we can drill down on these valuations to understand what is happening, and perhaps make some corrections to fix, or at the very least, avoid providing valuations when we know the model is most likely to have a high error?\n\nThe good news is that we can in fact drill down on these high-error units. To do so, we first calculate the shap_values for only the high-error valuation units. The below SHAP Summary Plot is produced considering *only* high-error valuations. From the SHAP Summary Plot we can see:\n\n- The feature that contributes most to high errors is `Price_p__median` which is an engineered (ie derived) feature built by looking at aggregate statistics about `comparable` units nearby the target property.\n- In general, many of the engineered features play a large role in the high error valuations, but it's important to note that they also increase the preformance of the model overall (ie. without them, the model would perform worse). \n\nHowever, the overall model Summary Plots(s) and the high-error Summary Plot are very similar, and it's difficult to infer exactly what's happening from the Summary Plots alone."],"metadata":{}},{"cell_type":"code","source":["shap.summary_plot(shap_values_misvalued_abs, X_misvalued_abs_t, features)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"image/png":"/plots/c30c9fad-0075-4583-ae95-3e5ae5d531d7.png"}}],"execution_count":26},{"cell_type":"markdown","source":["### Figure 6: Insights - Absolute Error %"],"metadata":{}},{"cell_type":"code","source":["plot_dependence = partial(\n    _dependence_plot, \n    feature_names=features,\n    category_map={},\n)\n\nplot_dependence(\n    [ \n     'TOTAL_FLOOR_AREA_e',\n     'NUMBER_HEATED_ROOMS_e', \n     'FLOOR_LEVEL_e', \n     'Price_p__median', \n     'Price_p__mean', \n     'TOTAL_FLOOR_AREA_e_minus_mean', \n     'NUMBER_HEATED_ROOMS_e_minus_mean',\n     'Latitude_m', \n     'Longitude_m', \n     'FLOOR_LEVEL_e_minus_mean'\n    ], \n    shap_values_misvalued_abs, \n    X_misvalued_abs_t,  \n    display_features = X_misvalued_abs_t,  \n    rotation=33,\n    figsize=(47.5, 40), \n    alpha=1, \n    x_jitter=0.5,\n    nrows=5,\n    ncols=2,\n    xlabelfontsize=24,\n    xtickfontsize=20,\n    xticklabelrotation=0,\n    ylabelfontsize=24,\n    ytickfontsize=21,\n    cbarlabelfontsize=22,\n    cbartickfontsize=20,\n    cbartickrotation=0\n)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"image/png":"/plots/45d2cd75-16ed-4315-b994-826c263eb52b.png"}}],"execution_count":28},{"cell_type":"markdown","source":["### Figure 7: High Error % Overestimation \n\nFrom the Summary Plot below we can see that the most important features influencing *overestimation* are very similary to the features influencing absolute error."],"metadata":{}},{"cell_type":"code","source":["shap.summary_plot(shap_values_misvalued_high, X_misvalued_high_t, features)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"image/png":"/plots/6b1e6fa3-f68f-4928-93db-d9adbb2662d5.png"}}],"execution_count":30},{"cell_type":"markdown","source":["### Figure 8: Insights - Overestimation Error \n\nSome interesting patterns with `A` and `B` that can lead to **overestimation error** misvaluation are:\n- a\n- b\n\nSome interesting patterns with `C` that can lead to **overestimation error** misvaluation are:\n- c\n\nSome interesting patterns with `D` that can lead to **overestimation error** misvaluation are:\n- d"],"metadata":{}},{"cell_type":"code","source":["plot_dependence = partial(\n    _dependence_plot, \n    feature_names=features,\n    category_map={},\n)\n\nplot_dependence(\n    [ \n     'TOTAL_FLOOR_AREA_e',\n     'NUMBER_HEATED_ROOMS_e', \n     'FLOOR_LEVEL_e', \n     'Price_p__median', \n     'Price_p__mean', \n     'TOTAL_FLOOR_AREA_e_minus_mean', \n     'NUMBER_HEATED_ROOMS_e_minus_mean',\n     'Latitude_m', \n     'Longitude_m', \n     'FLOOR_LEVEL_e_minus_mean'\n    ], \n    shap_values_misvalued_high, \n    X_misvalued_high_t,  \n    display_features = X_misvalued_high_t,  \n    rotation=33,\n    figsize=(47.5, 40), \n    alpha=1, \n    x_jitter=0.5,\n    nrows=5,\n    ncols=2,\n    xlabelfontsize=24,\n    xtickfontsize=20,\n    xticklabelrotation=0,\n    ylabelfontsize=24,\n    ytickfontsize=21,\n    cbarlabelfontsize=22,\n    cbartickfontsize=20,\n    cbartickrotation=0\n)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"image/png":"/plots/30a35136-cf1e-4a87-a099-b57a3fb65c5b.png"}}],"execution_count":32},{"cell_type":"markdown","source":["### Figure 9: Underestimation Error\n\nFrom the Summary Plot below we can see that the most important features influencing *underestimation* are very similary to the features influencing absolute error."],"metadata":{}},{"cell_type":"code","source":["shap.summary_plot(shap_values_misvalued_low, X_misvalued_low_t, features)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"image/png":"/plots/ebc35c0e-ddd5-4470-8f90-2e1e34220339.png"}}],"execution_count":34},{"cell_type":"markdown","source":["### Figure 10: Insights - Underestimation Error \n\nSome interesting patterns with `A` that can lead to **underestimation error** misvaluation are:\n- a\n\nSome interesting patterns with `B` that can lead to **underestimation error** misvaluation are:\n- b\n\nSome interesting patterns with `C` that can lead to **underestimation error** misvaluation are:\n- c\n\nSome interesting patterns with `D` that can lead to **underestimation error** misvaluation are:\n- d"],"metadata":{}},{"cell_type":"code","source":["plot_dependence = partial(\n    _dependence_plot, \n    feature_names=features,\n    category_map={},\n)\n\nplot_dependence(\n    [ \n     'TOTAL_FLOOR_AREA_e',\n     'NUMBER_HEATED_ROOMS_e', \n     'FLOOR_LEVEL_e', \n     'Price_p__median', \n     'Price_p__mean', \n     'TOTAL_FLOOR_AREA_e_minus_mean', \n     'NUMBER_HEATED_ROOMS_e_minus_mean',\n     'Latitude_m', \n     'Longitude_m', \n     'FLOOR_LEVEL_e_minus_mean'\n    ], \n    shap_values_misvalued_low, \n    X_misvalued_low_t,  \n    display_features = X_misvalued_low_t,  \n    rotation=33,\n    figsize=(47.5, 40), \n    alpha=1, \n    x_jitter=0.5,\n    nrows=5,\n    ncols=2,\n    xlabelfontsize=24,\n    xtickfontsize=20,\n    xticklabelrotation=0,\n    ylabelfontsize=24,\n    ytickfontsize=21,\n    cbarlabelfontsize=22,\n    cbartickfontsize=20,\n    cbartickrotation=0\n)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"image/png":"/plots/679f7b15-7ed9-4414-8cc9-a770e10b6437.png"}}],"execution_count":36},{"cell_type":"markdown","source":["### Figure 9: Interesting Feature Interactions\n\nSee the [Tree SHAP paper](https://arxiv.org/pdf/1802.03888.pdf) for more details, but briefly, [SHAP interaction values](https://christophm.github.io/interpretable-ml-book/shap.html#shap-interaction-values) are a generalization of SHAP values to higher order interactions. Fast exact computation of pairwise interactions are implemented in the latest version of XGBoost with the pred_interactions flag. With this flag XGBoost returns a matrix for every prediction, where the main effects are on the diagonal and the interaction effects are off-diagonal. The main effects are similar to the SHAP values you would get for a linear model, and the interaction effects captures all the higher-order interactions are divide them up among the pairwise interaction terms. Note that the sum of the entire interaction matrix is the difference between the model's current output and expected output, and so the interaction effects on the off-diagonal are split in half (since there are two of each). When plotting interaction effects the SHAP package automatically multiplies the off-diagonal values by two to get the full interaction effect.\n\nA summary plot of a SHAP interaction value matrix plots a matrix of summary plots with the main effects on the diagonal and the interaction effects off the diagonal."],"metadata":{}},{"cell_type":"code","source":["shap_interaction_values = explainer.shap_interaction_values(Xs)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":38},{"cell_type":"code","source":["\nshap.summary_plot(shap_interaction_values, Xs, \n                  max_display=15\n                 )\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"image/png":"/plots/8c2bd510-f92d-4ea5-a713-bd24389f5b99.png"}}],"execution_count":39},{"cell_type":"markdown","source":["### Figure 10: Interesting Interactions Cont'd\n\nNow we plot the interaction effects. These effects capture all of the vertical dispersion that was present in the original SHAP plot but is missing from the `main effects plot` (ie the dependence_plot with self-pair (`featureA`,`featureA`)). The plots below show that:.\n\n- Lower `A` interacts with `B` such that ...\n- Having a `C` is associated with lower probability of `D` when `E` is greater than approx. `X`"],"metadata":{}},{"cell_type":"code","source":["[ \n     'TOTAL_FLOOR_AREA_e',\n     'NUMBER_HEATED_ROOMS_e', \n     'FLOOR_LEVEL_e', \n     'Price_p__median', \n     'Price_p__mean', \n     'TOTAL_FLOOR_AREA_e_minus_mean', \n     'NUMBER_HEATED_ROOMS_e_minus_mean',\n     'Latitude_m', \n     'Longitude_m', \n     'FLOOR_LEVEL_e_minus_mean'\n    ]\n\nplot_dependence = partial(\n    _dependence_plot, \n    feature_names=features,\n    category_map={},\n)\n\nplot_dependence(\n    [('TOTAL_FLOOR_AREA_e', 'Price_p__median'),\n    ('NUMBER_HEATED_ROOMS_e', 'Price_p__median'), \n    ('FLOOR_LEVEL_e', 'Price_p__median'), \n    ('density_count', 'Price_p__median'), \n    ], \n    shap_interaction_values, \n    Xs, \n    figsize=(30,16.5), \n    rotation=15, \n    ncols=2, \n    nrows=2,\n    display_features=Xs,\n    xtickfontsize=20,\n    xlabelfontsize=20,\n    ylabelfontsize=20,\n    ytickfontsize=17,\n    cbarlabelfontsize=20,\n    cbartickfontsize=18,\n)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"image/png":"/plots/f92b2dfc-1f90-462f-911e-963cfc6cf0f5.png"}}],"execution_count":41},{"cell_type":"code","source":["import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n \n# Prepare the dataset\n# y_true, y_predicted\ntest = pd.DataFrame({\"Predicted\":y_predicted,\"Actual\":y_true})\ntest = test.reset_index()\ntest = test.drop([\"index\"], axis=1)\n \n# plot graphs\nfig= plt.figure(figsize=(16,8))\nplt.plot(test[:50])\nplt.legend([\"Actual\", \"Predicted\"])\nsns.jointplot(x=\"Actual\", y=\"Predicted\", data=test, kind=\"reg\");"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"image/png":"/plots/675017ec-8c9c-4d0c-80fd-9acb130779b5.png"}},{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"image/png":"/plots/51986ff9-252b-4fc9-98f4-cd584272cdd2.png"}}],"execution_count":42}],"metadata":{"name":"Model Explainability","notebookId":673632905022730},"nbformat":4,"nbformat_minor":0}