{"cells":[{"cell_type":"markdown","source":["# BCI AVM Hypertuning\n### Training the Machine Learning Models on Tabular Data: \n\nThis notebook covers the following steps:\n- Import data from AWS\n- Visualize the data using Seaborn and matplotlib\n- Run a parallel hyperparameter sweep to train machine learning models on the dataset\n- Explore the results of the hyperparameter sweep with MLflow\n- Register the best performing model in MLflow\n\n## Requirements\nThis notebook requires Databricks Runtime for Machine Learning or a similar spark/pyspark enabled environment setup locally (or elsewhere).  \nIf you are using Databricks Runtime 7.3 LTS ML or below, you must update the CloudPickle library using the commands in the following cell."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f775ddb6-7daf-4f9f-99fa-7fc024351b47"}}},{"cell_type":"code","source":["# These commands are only required if you are using a cluster running DBR 7.3 LTS ML or below. \nimport cloudpickle\nassert cloudpickle.__version__ >= \"1.4.0\", \"Update the cloudpickle library using `%pip install --upgrade cloudpickle`\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3eab982e-0862-4138-b473-53bfc84bcf48"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Importing Data\n  \nIn this section, you download a dataset from the web (AWS S3 Bucket).\n\n1. Ensure that you have installed `bciavm` using `pip install bciavm` in your local machine or on Databricks."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"366dccef-47e4-466b-af76-53e29ad99472"}}},{"cell_type":"code","source":["import io\nfrom bciavm.core.config import your_bucket\nfrom bciavm.utils.bci_utils import ReadParquetFile, get_postcodeOutcode_from_postcode, get_postcodeArea_from_outcode, drop_outliers, preprocess_data\nimport pandas as pd\nimport bciavm\nfrom sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Add Libraries","showTitle":true,"inputWidgets":{},"nuid":"555b7c1e-445b-4c43-b166-8ab6dcf27e32"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["dfPricesEpc = pd.DataFrame()\ndfPrices = pd.DataFrame()\n\nyearArray = ['2020', '2019']\nfor year in yearArray:\n    singlePriceEpcFile = pd.DataFrame(ReadParquetFile(your_bucket, 'epc_price_data/byDate/2021-02-04/parquet/' + year))\n    dfPricesEpc = dfPricesEpc.append(singlePriceEpcFile)\n\ndfPricesEpc['POSTCODE_OUTCODE'] = dfPricesEpc['Postcode'].apply(get_postcodeOutcode_from_postcode)\ndfPricesEpc['POSTCODE_AREA'] = dfPricesEpc['POSTCODE_OUTCODE'].apply(get_postcodeArea_from_outcode)\ndfPricesEpc.groupby('TypeOfMatching_m').count()['Postcode']"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Get Data","showTitle":true,"inputWidgets":{},"nuid":"a1e7323b-decf-43f0-ab1b-c3b5a5e0159e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Preprocessing Data\nPrior to training a model, check for missing values and split the data into training and validation sets."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27b6967c-5a25-4fe6-894c-4c07a63014d7"}}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = bciavm.preprocess_data(dfPricesEpc)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"601c2717-c8a5-4411-ba78-e033a9bcdd83"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##Hypertuning the AVM pipeline\n\nThe following code uses the `xgboost` and `scikit-learn` libraries to train a valuation model. It runs a parallel hyperparameter sweep to train multiple\nmodels in parallel, using Hyperopt and SparkTrials. The code tracks the performance of each parameter configuration with MLflow."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6bc8e62b-0ee7-42bf-838a-45c020259814"}}},{"cell_type":"code","source":["import sys\nfrom hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK\nfrom hyperopt.pyll import scope\nfrom math import exp\nimport mlflow\n\nversion_info = sys.version_info\nPYTHON_VERSION = \"{major}.{minor}.{micro}\".format(major=version_info.major,\n                                              minor=version_info.minor,\n                                              micro=version_info.micro)\n\n\nconda_env = {'channels': ['defaults','conda-forge'],\n            'dependencies': [\n                'python={}'.format(PYTHON_VERSION),\n                'pip',\n                  {'pip': ['bciavm==1.21.2',\n                          ],\n                  },\n            ],\n            'name': 'mlflow-env'\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Create a conda env log","showTitle":true,"inputWidgets":{},"nuid":"01f69756-98aa-4e0e-b8bf-0a7d84dbdb05"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["import gc\nfrom bciavm.pipelines import RegressionPipeline\nimport numpy as np\n\n\nsearch_space = {\n  #Transformer tuning\n  'numeric_impute_strategy': hp.choice('numeric_impute_strategy', [\"mean\", \"median\", \"most_frequent\"]),\n  'top_n': hp.choice('top_n', [1, 5, 10, 20, 30]),\n    \n  #K nearest neighbor tuning\n  'n_neighbors': scope.int(hp.quniform('n_neighbors', 2, 30, 1)),\n  'leaf_size': hp.choice('leaf_size', [5, 10, 20, 30]),\n  'p': hp.choice('p', [1, 2, 3]),\n    \n  #MultiLayer Perceptron Regressor tuning\n  'activation': 'relu',\n  'solver': 'adam',\n  'batch_size': scope.int(hp.quniform('batch_size', 200, 1000, 10)),\n  'alpha': hp.loguniform('alpha', -5, -1),\n  'learning_rate_init': hp.loguniform('learning_rate_init', -3, 0),\n  'max_iter': scope.int(hp.quniform('max_iter', 100, 500, 10)),\n  'beta_1': hp.choice('beta_1', [0.1, 0.2, 0.4, 0.6, 0.8, 0.9]),\n  'epsilon': hp.choice('epsilon', [1e-08, 1e-07, 1e-06, 1e-05, 1e-04, 1e-09, 1e-03]),\n    \n  #XGBoost Regressor tuning\n  'n_estimators': scope.int(hp.quniform('n_estimators', 100, 1000, 10)),\n  'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n  'learning_rate': hp.loguniform('learning_rate', -3, 0),\n  'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n  'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n  'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n  'metric':'mae',\n  'objective': 'reg:squarederror',\n  'seed': 123, # Set a seed for deterministic training\n}\n\n\ndef train_model(params):\n  \n    # With MLflow autologging, hyperparameters and the trained model are automatically logged to MLflow.\n    mlflow.sklearn.autolog()\n\n    with mlflow.start_run(nested=True):\n\n        parameters = {\n              'Imputer': {'categorical_impute_strategy': 'most_frequent',\n                'numeric_impute_strategy': params['numeric_impute_strategy'],\n                'categorical_fill_value': None,\n                'numeric_fill_value': None},\n               'One Hot Encoder': {'top_n': params['top_n'],\n                'features_to_encode': ['agg_cat'],\n                'categories': None,\n                'drop': None,\n                'handle_unknown': 'ignore',\n                'handle_missing': 'error'},\n               'MultiLayer Perceptron Regressor': {'activation': 'relu',\n                'solver': 'adam',\n                'alpha': params['alpha'],\n                'batch_size': params['batch_size'],\n                'learning_rate': 'constant',\n                'learning_rate_init': params['learning_rate_init'],\n                'max_iter': params['max_iter'],\n                'early_stopping': True,\n                'beta_1': params['beta_1'],\n                'beta_2': 0.999,\n                'epsilon': params['epsilon'],\n                'n_iter_no_change': 10},\n               'K Nearest Neighbors Regressor': {'n_neighbors': params['n_neighbors'],\n                'weights': 'distance',\n                'algorithm': 'auto',\n                'leaf_size': params['leaf_size'],\n                'p': params['p'],\n                'n_jobs': 4},\n               'XGBoost Regressor': {'learning_rate': params['learning_rate'],\n                'max_depth': params['max_depth'],\n                'min_child_weight': params['min_child_weight'],\n                'reg_alpha': params['reg_alpha'],\n                'reg_lambda': params['reg_lambda'],\n                'n_estimators': params['n_estimators']},\n        }\n\n        class AVMPipeline(RegressionPipeline):\n            custom_name = 'AVM Pipeline'\n            component_graph = {\n                \"Preprocess Transformer\": [\"Preprocess Transformer\"],\n                'Imputer': ['Imputer', \"Preprocess Transformer\"],\n                'One Hot Encoder': ['One Hot Encoder', \"Imputer\"],\n                'MultiLayer Perceptron Regressor': ['MultiLayer Perceptron Regressor',  \n                                                    'One Hot Encoder'],\n                'K Nearest Neighbors Regressor': ['K Nearest Neighbors Regressor', \n                                                  'MultiLayer Perceptron Regressor', \n                                                  'One Hot Encoder'],\n                'XGBoost Regressor': [\"XGBoost Regressor\", \n                                    'K Nearest Neighbors Regressor', \n                                    'One Hot Encoder']\n            }\n            \n        for component in parameters:\n          for param in parameters[component]:\n            _component_param = component + '_' + param\n            mlflow.log_param(_component_param, parameters[component][param])\n\n        avm_pipeline = AVMPipeline(parameters=parameters)\n        avm_pipeline.fit(X_train, y_train)\n\n        # Compute and return trial error\n        scores = avm_pipeline.score(X_test, \n                                         y_test, \n                                         objectives=['MAPE',\n                                                   'MdAPE',\n                                                   'ExpVariance',\n                                                   'MaxError',\n                                                   'MedianAE',\n                                                   'MSE',\n                                                   'MAE',\n                                                   'R2',\n                                                   'Root Mean Squared Error'])\n        MdAPE = scores['MdAPE']\n\n        #Examine the learned feature importances output by the model as a sanity-check.\n        fi = pd.DataFrame({'feature':avm_pipeline.get_component(\"XGBoost Regressor\").input_feature_names,'importance':avm_pipeline.get_component(\"XGBoost Regressor\").feature_importance}).sort_values(by='importance', ascending=False)\n\n        #Log the feature importances output as an artifact\n        artifact_path = '/dbfs/FileStore/tables/XGBoost_importance.csv'\n        fi.to_csv(artifact_path,index=False)\n        mlflow.log_artifact(artifact_path)\n\n        #Log the scoring metrics\n        mlflow.log_metric('MAPE', scores['MAPE'])\n        mlflow.log_metric('MdAPE', scores['MdAPE'])\n        mlflow.log_metric('ExpVariance', scores['ExpVariance'])\n        mlflow.log_metric('MaxError', scores['MaxError'])\n        mlflow.log_metric('MedianAE', scores['MedianAE'])\n        mlflow.log_metric('MSE', scores['MSE'])\n        mlflow.log_metric('MAE', scores['MAE'])\n        mlflow.log_metric('R2', scores['R2'])\n        mlflow.log_metric('Root Mean Squared Error', scores['Root Mean Squared Error'])\n\n        #Log an input example for future reference\n        input_example = X_train.dropna().sample(1)\n\n        #Log the mlflow model, along with the conda environment and input example\n        mlflow.sklearn.log_model(\n                             avm_pipeline,\n                             \"avm\", \n                             conda_env=conda_env,\n                             input_example=input_example\n                            )\n\n        # fmin minimizes the MdAPE (median absolute percentage error)\n        return {'status': STATUS_OK, 'loss': scores['MdAPE']}\n\n# Greater parallelism will lead to speedups, but a less optimal hyperparameter sweep. \n# A reasonable value for parallelism is the square root of max_evals.\nspark_trials = SparkTrials(parallelism=10)\n\n# Run fmin within an MLflow run context so that each hyperparameter configuration is logged as a child run of a parent\nwith mlflow.start_run(run_name='pipeline_tuning'):\n    best_params = fmin(\n        fn=train_model, \n        space=search_space, \n        algo=tpe.suggest, \n        max_evals=100,\n        trials=spark_trials,\n        rstate=np.random.RandomState(123)\n        )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"179360bb-0aa0-4de4-a224-7d4b48340c40"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Use MLflow to view the results\nOpen up the Experiment Runs sidebar to see the MLflow runs. Click on Date next to the down arrow to display a menu, and select 'MdAPE' to display the runs sorted by the MdAPE metric. The lowest MdAPE value is ~10%. \n\nMLflow tracks the parameters and performance metrics of each run. Click the External Link icon <img src=\"https://docs.databricks.com/_static/images/external-link.png\"/> at the top of the Experiment Runs sidebar to navigate to the MLflow Runs Table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"012e3c74-236f-4166-bb15-b129eb01181a"}}},{"cell_type":"markdown","source":["Now investigate how the hyperparameter choice correlates with MdAPE. Click the \"+\" icon to expand the parent run, then select all runs except the parent, and click \"Compare\". Select the Parallel Coordinates Plot.\n\nThe Parallel Coordinates Plot is useful in understanding the impact of parameters on a metric. You can drag the pink slider bar at the upper right corner of the plot to highlight a subset of MdAPE values and the corresponding parameter values. The plot below highlights the highest MdAPE values:\n\n<img src=\"https://docs.databricks.com/_static/images/mlflow/end-to-end-example/parallel-coordinates-plot.png\"/>\n\nNotice that all of the top performing runs have a low value for reg_lambda and learning_rate. \n\nYou could run another hyperparameter sweep to explore even lower values for these parameters. For simplicity, that step is not included here."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00e5ea89-0918-44b3-9fca-60f7f48fb6b5"}}},{"cell_type":"markdown","source":["You used MLflow to log the model produced by each hyperparameter configuration. The following code finds the best performing run and saves the model to the model registry."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1752fa82-55e2-457c-b6d3-d545483b1eae"}}},{"cell_type":"code","source":["best_run = mlflow.search_runs(order_by=['metrics.MdAPE DESC']).iloc[0]\nprint(f'MdAPE of Best Run: {best_run[\"metrics.MdAPE\"]}')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac2d08c5-5381-40e8-b8a2-a0737ca5f934"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Updating the production wine_quality model in the MLflow Model Registry\n\nEarlier, we saved the model to the Model Registry under \"avm\". Now that we have a created a more accurate model, update avm."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c34b4cf8-35f1-48e8-bc64-51b6b4cf0f70"}}},{"cell_type":"code","source":["model_name = 'avm'\nnew_model_version = mlflow.register_model(f\"runs:/{best_run.run_id}/model\", model_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7419396-4da9-4181-8a3d-613b80d3544b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Click **Models** in the left sidebar to see that the wine_quality model now has two versions. \n\nThe following code promotes the new version to production."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"faaceaa1-fb8c-4e72-80c6-b9c7c5815a18"}}},{"cell_type":"code","source":["# Archive the old model version\nclient.transition_model_version_stage(\n  name=model_name,\n  version=model_version.version,\n  stage=\"Archived\"\n)\n\n# Promote the new model version to Production\nclient.transition_model_version_stage(\n  name=model_name,\n  version=new_model_version.version,\n  stage=\"Production\"\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49c2eaec-eaf5-4e38-ab58-1d029271e4b9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Clients that call load_model now receive the new model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11553e11-faea-43ae-aac1-10545a252e64"}}},{"cell_type":"code","source":["# This code is the same as the last block of \"Building a Baseline Model\". No change is required for clients to get the new model!\nmodel = mlflow.pyfunc.load_model(f\"models:/{model_name}/production\")\nprint(f'MdAPE: {roc_auc_score(y_test, model.predict(X_test))}')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c7a24e2-9eb2-432b-af35-e3bf3dc03a46"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Hypertuning","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2591899835818568}},"nbformat":4,"nbformat_minor":0}
